<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python常用内置模块总结]]></title>
    <url>%2F2018%2F03%2F03%2Fpython%E5%B8%B8%E7%94%A8%E5%86%85%E7%BD%AE%E6%A8%A1%E5%9D%97%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[下面是Python的一些常用内置模块简单使用示例, 主要包括: 文件操作模块 os, glob, shutil 时间模块 time 和 datetime 序列化模块 pickle 和 json 哈希算法模块 hashlib 和 hmac 数据结构模块 collections 随机数模块 random 字节流转换模块 struct 系统信息模块 psutil 1. os, glob, shutil12345678910111213141516171819202122232425262728293031import osimport globimport shutilos.uname() # 查看操作系统详细信息os.getcwd() # 获取当前工作目录os.system('command') # 运行shell命令os.listdir(dirname) # 列出目录下所有的目录和文件os.chdir('path') # 切换当前工作目录os.mkdir('path') # 新建目录os.rmdir('path') # 删除目录os.remove('file_path') # 删除文件os.rename('old_filename', 'new_filename') # 重命名文件shutil.copyfile(src_file, des_file) # 拷贝文件shutil.move(src_file, des_path) # 移动文件os.getpid() # 查看当前进程idos.getppid() # 查看父进程idglob.glob('*.py') # 筛选当前目录特定文件os.path.isdir('path') # 判断是否为目录os.path.isfile('path') # 判断是否为文件os.path.exists('path') # 判断路径是否存在os.path.abspath() # 获取绝对路径os.path.normpath('path') # 规范路径名os.path.join('path1','path2') # 连接目录与文件名os.path.split('path') # 目录名 + 文件名os.path.splitext('path') # 前缀 + 拓展名os.path.dirname('path') # 上级路径名os.path.basename('path') # 当前目录名或文件名os.path.getsize('path') # 获取文件大小 2. time, datetimetime module转换图示 代码实例123456789# timestamp, struct_time, format_timeimport timetime.time() # 生成timestamptime.localtime() # 生成struct_timetime.mktime(time.localtime()) # struct_time to timestamptime.localtime(time.time()) # timestamp to struct_timetime.strptime('2011-05-05 16:37:06', '%Y-%m-%d %X') # format_time to struct_timetime.strftime("%Y-%m-%d %X") # struct_time to format_timetime.strftime("%Y-%m-%d %X", time.localtime()) # struct_time to format_time datetime module1234567891011121314151617181920212223import time as ttimefrom datetime import date, time, datetime, timedeltad = date.fromtimestamp(ttime.time())d = date.today() # dated.year, d.month, d.day # attrsd.isoformat() # YYYY-MM-DDd.strftime('%Y-%m-%d') # date to format_datet = time(23, 46, 10) # construct timet.hour, t.minute, t.second # attrst.isoformat() # HH:MM:SSt.strftime('%X') # date to format_timedt = datetime.now() # get datetimedt.ctime() # datetime to ctimedt.isoformat() # datetime to format_timedt.strftime('%Y-%m-%d %H:%M:%S') # datetime to format_timedt + timedelta(days=2, hours=-8)ts = dt.timestamp() # datetime to timestampdatetime.fromtimestamp(ts) # timestamp to datetimedatetime.combine(date, time) # combine date and timedatetime.strptime('2018-03-01 18:19:59', '%Y-%m-%d %H:%M:%S') # format_time to datetime 3. pickle, jsonpickle module12345678910111213141516import pickle# 使用pickle模块将数据对象保存到文件data1 = &#123;'a': [1, 2.0, 3, 4+6j], 'b': ('string', u'Unicode string'), 'c': None&#125;lst = [1, 2, 3]lst.append(lst)output = open('data.pkl', 'wb')pickle.dump(data1, output) # Pickle dictionary using protocol 0.pickle.dump(lst, output, -1) # Pickle the list using the highest protocol available.output.close()#使用pickle模块从文件中重构python对象pkl_file = open('data.pkl', 'rb')data1 = pickle.load(pkl_file)data2 = pickle.load(pkl_file)pkl_file.close() json module12345678910111213141516171819202122232425262728import jsonclass Student(object): def __init__(self, name, age, score): self.name = name self.age = age self.score = scoredef student2dict(std): return &#123; 'name': std.name, 'age': std.age, 'score': std.score &#125;def dict2student(d): return Student(d['name'], d['age'], d['score'])# json字符串s = Student('Bob', 20, 88)json_str = json.dumps(s, default=student2dict)json.loads(json_str, object_hook=dict2student)# json文件with open('data.json', 'w') as f: json.dump(data, f)with open('data.json', 'r') as f: data = json.load(f) Note:&nbsp;json支持的数据类型为None, bool, int, float, str以及这些类型数据的list, tuple and dict 4. hashlib, hmac1234567891011121314151617import hmacimport hashlibmd5 = hashlib.md5() # 128个字节,用32位的16进制表示md5.update('how to use md5 in '.encode('utf-8'))md5.update('python hashlib?'.encode('utf-8'))print(md5.hexdigest())sha1 = hashlib.sha1()# 160个字节,用40位的16进制表示sha1.update('how to use sha1 in '.encode('utf-8'))sha1.update('python hashlib?'.encode('utf-8'))print(sha1.hexdigest())message = b'Hello, world!'key = b'secret' # key必须为二进制h = hmac.new(key, message, digestmod='MD5')print(h.hexdigest()) 5.collectionsnamedtuple12345678from collections import namedtuplePoint = namedtuple('Point', ['x', 'y'])Circle = namedtuple('Circle', ['x', 'y', 'r'])p = Point(1,2) # or Point._make((1,2))print(p.x, p.y)True == isinstance(p, Point)True == isinstance(p, tuple) deque12from collections import dequedeque.appendleft()/append()/popleft()/pop() defaultdict12from collections import defaultdictdd = defaultdict(lambda: 'N/A') # key不存在时访问返回默认值 OrderedDict1234from collections import OrderedDictd = dict([('a', 1), ('b', 2), ('c', 3)])od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])print(d, '\n', od) Counter12345678910111213141516171819202122232425262728293031323334353637from collections import Counter # dict的子类# 创建lst = [1, 2, 3, 4, 1, 3]con = Counter(lst)# 访问con[1] =&gt; 2con[5] =&gt; 0# 更新c.update(lst)con[1] =&gt; 4c.subtract(lst)con[1] =&gt; 2# 删除del con[1]# elements()con.elements() =&gt; [2, 3, 3, 4]# most_commoncon.most_common(1) =&gt; [(3, 2)]# 算数和集合操作c = Counter(a=3, b=1)d = Counter(a=1, b=2)c + dc - dc &amp; dc | d# 其它con = Counter('a'=2, 'b'=0, 'c'=-1, 'd'=4)con += Counter()con =&gt; &#123;'a': 2, 'd': 4&#125; 6. random123456789import randomrandom.random() # 生成[0, 1)浮点数random.uniform(a, b) # 生成[a, b]之间的随机浮点数random.randint(a, b) # 生成[a, b]之间的随机整数random.randrange(a, b, step) # 从[a, b)范围内, 返回按指定基数递增的集合中的一个随机数random.choice(sequence) # 从指定序列中返回一个随机元素random.sample(sequence, num) # 从指定序列中随机获取指定长度的片段random.shuffle(lst) # 将指定列表中的元素打乱 7. struct1234567import struct# 完成bytes和其它任意二进制数据类型的转换struct.pack('&gt;IH', b'\xf0\xf0\xf0\xf0\x80\x80')res: (4042322160, 32896)struct.unpack('&lt;ccIIIIIIHH', b'\x42\x4d\x38\x8c\x0a\x00\x00\x00\x00\x00\x36\x00\x00\x00\x28\x00\x00\x00\x80\x02\x00\x00\x68\x01\x00\x00\x01\x00\x18\x00')res: (b'B', b'M', 691256, 0, 54, 40, 640, 360, 1, 24) 8. psutil12345678910111213141516171819202122232425# CPU信息psutil.cpu_count()psutil.cpu_count(logical=False)psutil.cpu_times()psutil.cpu_percent(interval=1, percpu=True)# 内存信息psutil.virtual_memory()psutil.swap_memory()# 磁盘信息psutil.disk_partitions()psutil.disk_usage('/')psutil.disk_io_counters()# 网络信息psutil.net_io_counters()psutil.net_if_addrs()psutil.net_if_stats()psutil.net_connections()# 进程信息psutil.pids()p = psutil.Process(3776)p.name() and etc]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python_module</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记 -- gemini双核双系统]]></title>
    <url>%2F2018%2F01%2F21%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-gemini%E5%8F%8C%E6%A0%B8%E5%8F%8C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[一、背景知识IPI中断机制多核多线程处理器的中断由 PIC(Programmable Interrupt Controller）统一控制，PIC 允许一个硬件线程中断其他的硬件线程，这种方式被称为核间中断(Inter-Processor Interrupts，IPI）。 PIC 拥有一个宽度为 32 位的核间中断寄存器IPIBase，该寄存器包含目的线程的编号、中断向量及中断类型(是否中断多个硬件线程）等内容。核间中断可以通过向这个寄存器写入需要的值来产生。若硬件线程A想要发送一个核间中断给硬件线程B，它只需要向寄存器IPIBase中写入B的Thread ID、中断向量、中断类型等值就可以了，PIC会通知B所在的核挂起它当前的执行序列，并根据中断向量跳转到中断服务例程ISR的入口。 使用IPI进行核间通信的关键在于要利用中断服务例程ISR去读取一个事先约好的共享内存区域。发起方首先将消息写到一块共享内存中，然后发起核间中断。被中断的硬件线程在中断服务例程中读取该内存，以获得发起方通知的消息。为防止多核间的竞争导致消息被改写，使用这种方式必须利用锁机制来确保消息的完整性。 WFI和WFE模式WFI 表示 Wait for Interrupt（等待中断） ， WFE 表示 Wait for Event（等待事件）， 这两个指令允许处理器进入低功耗模式和停止执行代码。 Buildroot开发环境Buildroot 是嵌入式开发领域中一个成套的嵌入式开发环境， 通过交叉编译，Buildroot 大大简化了制作一个完整的嵌入式 linux 开发系统的流程。Buildroot 可以生成交叉编译工具链、 根文件系统、 linux 内核镜像以及buildloader 等，当然也可以独立生成其中某些组合项， 比如你可以利用现有的交叉编译工具制作你自己的根文件系统。 此外， buildroot 支持 Power PC、 MIPS、ARM 等硬件平台， 在嵌入式领域应用非常广泛。 二、系统架构如上图所示，Linux启动时占用两个处理器，然后采用hotplug技术拔掉CPU#1，重新配置CPU#1启动地址，加载并启动RT-Thread，两个系统独立运行在自己的处理器上，通过VBus进行通信。 三、内存分配双核双系统内存分配如上图，基于qemu仿真器模拟运行。总共分配256M内存空间，其中248M分配给Linux，顶端4M作为RT-Thread的可执行空间，也是RT-Thread的加载位置。与RT-Thread相邻的1M空间用于加载boot.bin，这个文件用于启动RT-Thread，类似于bootloader，临近Linux的2M空间作为VBus通信的共享内存空间。此外，图中还给出了物理地址和虚拟地址之间的1:1映射关系。 四、基本流程双系统基本操作流程如上图，详细介绍如下： 启动Linux采用SMP方式，启动Linux，并占用两个处理器 加载RT-Thread将boot.bin和rtthread.bin分别加载到预先指定指定位置0x6fb0 0000、0x6fc 0000，实际上是加载到对应的虚拟地址空间0x8fb0 0000、0x8fc 0000 拔出CPU#1进入WFI模式通过hotplug技术，调用API 唤醒cpu_down，拔除CPU#1，使其进入低功耗模式 配置CPU#1启动地址设置CPU#1的启动地址为0x6fb0 0000 发送IPI中断，唤醒CPU#1从CPU#0发送一个IPI中断到CPU#1，唤醒CPU#1，启动RT-Thread 双系统之间通信双系统之间通过VBus进行通信 五、Linux拔核cpu_down的基本流程如上图，下面详细介绍cpu_down的基本流程: “cpu_down” (in kernel/cpu.c) 调用cpu_maps_update_begin，设定 Mutex Lock “cpu_add_remove_lock”； 确认cpu_hotplug_disabled是否有被设定； 调用 _cpu_down(cpu, 0)； 调用cpu_maps_update_done，释放 Mutex Lock “cpu_add_remove_lock”。 “_cpu_down” (in kernel/cpu.c) 调用num_online_cpus，确认如果目前Online的处理器只有一个，直接返回错误； 调用cpu_online，如果该CPU并非Online状态，直接返回错误； 调用cpu_hotplug_begin，取得 Mutex Lock “cpu_hotplug.lock”； 调用cpu_notify，通过raw_notifier_call_chain，通知CPU Chain中的处理器，目前正在进行Online动作的处理器状态为”CPU_DOWN_PREPARE”； 调用函数__stop_machine； 通过BUG_ON(cpu_online(cpu))，确认要停止的处理器，是否已经处于Offline的状态，若还是在Online状态就会导致Kernel Panic； 调用函数idle_cpu (in kernel/shced.c)，确认要Offline处理器是否正在执行idle task.若该处理器不是正在执行Idle Task，就会调用cpu_relax，直到确认要Offline的处理器是处于Idle Task中； 调用__cpu_die(cpu)； 调用cpu_notify_nofail，通知完成Offline动作的处理器状态为”CPU_DEAD”； 调用check_for_tasks，确认目前是否还有Tasks在被停止的处理器上，若有就会Printk出警告讯息； 调用cpu_hotplug_done，设定Active Write为NULL，释放 Mutex Lock “cpu_hotplug.lock”。 “__cpu_die” (in arch/arm/kernel/smp.c) 执行函数wait_for_completion_timeout，等待函数cpu_die 通过函数complete设定”Completion”给 cpu_died物件，如果cpu_died物件有设定完成或是TimeOut就会继续往后执行； 调用platform_cpu_kill。 ”vexprss_cpu_die”(in arch/arm/mach-vexpress/hotplug.c) CPU Idle Task在执行cpu_die后，就会进入函数platform_cpu_die，并通过platform_do_lowpower，让处理器处于WFI Low Power的状态，等待下一次的唤醒。若处理器重新被唤醒，就会执行函数secondary_start_kernel (in arch/arm/kernel/smp.c)，重新执行初始化流程。 下面简单介绍一下拔核的一种实现方式，添加Linux内核驱动模块：12345678910111213141516171819#include &lt;linux/module.h&gt;#include &lt;linux/cpu.h&gt;static int __init unplug_init(void)&#123; int ret; ret = cpu_down(1); if (ret &amp;&amp; (ret != -EBUSY)) &#123; pr_err("Can't release cpu1: %d\n", ret); return -ENOMEM; &#125; return 0;&#125;static void __exit unplug_exit(void)&#123;&#125;module_init(unplug_init);module_exit(unplug_exit); 上面代码即为Linux添加一个简单的内核驱动模块，实现拔核功能。类似于C++面向对象编程中类的实现，编写内核驱动模块代码也需要编写”构造函数”、”析构函数”，cpu_down的调用即在”构造函数”中完成，当加载unplug模块的同时，也完成了拔核。编译命令1make -C $KDIR M=$PWD ARCH=arm 其中 \$KDIR 为Linux源码根目录，\$PWD为驱动模块根目录，在驱动模块根目录下执行这条命令即可，参数的含义请自行google。 六、VBus通信双核双系统的VBus通信组件如上图所示，VBus是建立在环形缓冲区上的一个组件，用于高效的进行系统间通讯，具有如下特点: 支持QoS机制，保证关键数据及时送达 支持多路复用，可以在一条VBus上实现多种功能 直接使用设备接口和文件接口，方便易用 Linux侧支持用户态驱动程序，可靠性高 根据VBus的特点可知，Linux用户态程序可以像操作普通文件一样来通过VBus来进行通讯，支持常规的open/read/write/close等基本操作，RT-Thread程序可以像操作普通设备一样通过VBus来通讯，包括rt_device{open/read/write/close}等操作。 最后，给出VBus的基本实现原理，如上图，感兴趣的话可以进一步学习。 七、测试运行运行Linux1234567891011qemu-system-arm \ -M vexpress-a9 \ -cpu cortex-a9 \ -smp cpus=2 \ -m 256M \ -kernel zImage \ -drive file=rootfs.ext2,if=sd \ -append &quot;mem=248M console=ttyAMA0 root=/dev/mmcblk0&quot; \ -dtb vexpress-v2p-ca9.dtb \ -serial stdio \ -serial telnet:127.0.0.1:1200,server 基本参数和Linux方针运行类似，注意最后两项，分别将Linux和RT-Thread重定向到标准输入输出、telnet服务器。 运行RT-Thread1telnet 127.0.0.1 1200 运行结果启动Linux 查看CPU运行情况 加载rtloader模块 查看CPU运行情况 加载过程 由上图可见双核上系统的基本启动流程，首先分别加载boot.bin和rtthread.bin到指定物理位置，然后调用cpu_down拔除CPU#1，配置好启动地址后，发送IPI中断唤醒CPU#1，同时为Linux加载VBus driver，最终成功启动双核双系统。通信测试 核间通信功能：Linux发送一个字符串，可有空格分隔符，RT-Thread收到后反序回写，Linux端收到处理结果并打印。 八、附录1. gemini 双核双系统调整版 九、参考链接1. gemini 双核双系统原版 声明：gemini双核双系统主体代码均由RT-Thread作者实现，博主仅在其基础上做出些许改变，关键实现代码添加了相关注释，本文即对gemini双核双系统基本原理的介绍，也仅仅基于我个人的理解，其中必然由不准确甚至错误的地方，仅作参考！]]></content>
      <categories>
        <category>Linux学习笔记</category>
      </categories>
      <tags>
        <tag>RT-Thread</tag>
        <tag>Linux</tag>
        <tag>gemini</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记 -- qemu仿真运行linux]]></title>
    <url>%2F2018%2F01%2F20%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-qemu%E4%BB%BF%E7%9C%9F%E8%BF%90%E8%A1%8Clinux%2F</url>
    <content type="text"><![CDATA[一、背景知识qemu仿真器qemu是一套由Fabrice Bellard所编写的模拟处理器的自由软件，它与Bochs，PearPC近似，但其具有某些后两者所不具备的特性，如高速度及跨平台的特性。经由KVM（早期为kqemu加速器，现在kqemu已被KVM替换）这个开源的加速器，QEMU能模拟至接近真实电脑的速度。qemu有两种主要运作模式： user mode，亦即是用户模式。QEMU能引导那些为不同中央处理器编译的Linux程序。而Wine及Dosemu是其主要目标； System mode，亦即是系统模式。QEMU能模拟整个电脑系统，包括中央处理器及其他周边设备。它使得为系统源代码进行测试及除错工作变得容易。其亦能用来在一部主机上虚拟数部不同虚拟电脑。 交叉编译工具链与gcc编译器不同的地方是，gcc编译器依赖于硬件平台类型，gcc编译的二进制文件只能在相应的硬件平台上运行，而交叉编译工具链的跨平台特性，使得其可以为不同的硬件平台生成二进制文件。 二、工具准备下载并解压linux、busybox、u-boot源码12345678wget https://www.kernel.org/pub/linux/kernel/v4.x/linux-4.4.1.tar.xzwget http://www.busybox.net/downloads/busybox-1.25.1.tar.bz2wget https://github.com/u-boot/u-boot/archive/v2017.11.tar.gzmv v2017.11.tar.gz u-boot-2017.11.tar.gztar -xvf packages/linux-4.4.1.tar.xztar -jxvf packages/busybox-1.25.1.tar.bz2tar -xzvf packages/u-boot-2017.11.tar.gz 安装qemu、32位运行环境1sudo apt-get install -y qemu libncurses5-dev zip bc texinfo 安装交叉编译工具链1sudo apt-get install -y gcc-arm-linux-gnueabi 三、编译源码编译linux内核1234export ARCH=arm # 指定核心类型export CROSS_COMPILE=arm-linux-gnueabi- # 指定交叉编译前缀make vexpress_defconfig # 使用预配置文件make zImage dtbs modules -j8 # 编译 拷贝生成文件到指定文件夹中12cp arch/arm/boot/zImage ../extra_folder/cp arch/arm/boot/dts/*ca9.dtb ../extra_folder/ 四、制作根文件系统根文件系统的目录结构 /bin 存放二进制可执行命令的目录 /dev 存放设备文件的目录 /etc 存放系统管理和配置文件的目录 /home 用户主目录，比如用户user的主目录就是 /home/user，可以用~user表示 /lib 存放动态链接共享库的目录 /sbin存放系统管理员使用的管理程序的目录 /tmp 公用的临时文件存储点 /root 系统管理员的主目录 /mnt 系统提供这个目录是让用户临时挂载其他的文件系统。 /proc 虚拟文件系统，可直接访问这个目录来获取系统信息。 /var 某些大文件的溢出区 /usr 最庞大的目录，要用到的应用程序和文件几乎都在这个目录。 对于经常使用Linux系统的读者来说，这些目录大部分应该很熟悉了。不过有几个目录对初学者来说容易混淆，如/bin，/sbin，/usr/bin和/usr/sbin。这里简单介绍一下它们的区别：/bin目录一般存放对于用户和系统来说都是必须的二进制文件，而/sbin目录要存放的是只针对系统管理的二进制文件，该目录的文件将不会被普通用户使用。相反，那些不是必要的用户二进制文件存放在/usr/bin下面，那些不是非常必要的系统管理工具放在/usr/sbin下。此外，对于一些本地的库也非常类似，对于那些要求启动系统和运行的必须命令要存放在/lib目录下，而对于其他不是必须的库存放在/usr/lib目录就可以。 生成busybox运行指令1234export ARCH=ARMexport CROSS_COMPILE=arm-linux-gnueabi-make defconfigmake install -j8 创建文件系统目录1234cd _installmkdir -p dev etc lib proc sys mnt tmp varmkdir -p etc/init.dcd .. 添加/etc/配置文件12345git clone https://github.com/mahadevvinay/Embedded_Linux_Files.git ../Embedded_Linux_Filessudo cp ../Embedded_Linux_Files/fstab _install/etc/sudo cp ../Embedded_Linux_Files/inittab _install/etc/sudo cp ../Embedded_Linux_Files/rcS _install/etc/init.d/sudo rm -rf ../Embedded_Linux_Files/ 修改可执行文件权限12sudo chmod a+x _install/etc/init.d/rcSsudo chmod 777 _install/etc/init.d/rcS 安装modules123cd ../linux-4.4.1/make modules_install ARCH=arm INSTALL_MOD_PATH=../busybox-1.25.1/_installcd ../busybox-1.25.1/ 拷贝交叉编译工具链运行库1sudo cp -P /usr/arm-linux-gnueabi/lib/* _install/lib/ 创建4个tty终端设备1234sudo mknod _install/dev/tty1 c 4 1sudo mknod _install/dev/tty2 c 4 2sudo mknod _install/dev/tty3 c 4 3sudo mknod _install/dev/tty4 c 4 4 制作根文件系统镜像123456dd if=/dev/zero of=a9rootfs.ext3 bs=1M count=32mkfs.ext3 a9rootfs.ext3sudo mkdir -p tmpfssudo mount -t ext3 a9rootfs.ext3 tmpfs/ -o loopsudo cp -r _install/* tmpfs/sudo umount tmpfs 拷贝生成的文件系统到目标文件夹中1cp a9rootfs.ext3 ../extra_folder/ 五、启动网络支持工具准备12sudo apt-get install uml-utilitiessudo apt-get install bridge-utils 查看本地网络链接1ifconfig 博主的网络连接状况如下:其中第一个是有线网连接，目前尚未联网，第三个是无线网连接，后面将使用这个网络。 建立桥接1sudo atom /etc/network/interfaces 添加以下内容123auto br0iface br0 inet dhcp bridge_ports wlp4s0 其中 wlp4s0 换成你自己的网络连接名字即可。 使修改立即生效1sudo /etc/init.d/networking restart 六、启动tftp服务工具准备12sudo apt-get install tftp-hpa # 客户端软件sudo apt-get install tftpd-hpa # 服务程序 建立tftp服务器工作目录12mkdir -p /path/to/tftpbootchmod 777 /path/to/tftpboot # 允许其它主机上传或者下载文件 设置tftp服务器工作目录12sudo atom /etc/default/tftpd-hpa修改 TFTP_DIRECTORY=&quot;/path/to/tftpboot&quot; 开启tftp服务1service tftpd-hpa restart 测试tftp服务123touch /path/to/tftpboot/testtftp 172.20.94.226&gt; get test 其中ip地址换成你自己的即可，如果当前目录下能够得到test文件，说明成功了。 七、编译u-boot修改参数配置文件1sudo atom /include/configs/vexpress_common.h 修改相应部分代码如下12345678#define CONFIG_BOOTCOMMAND \ &quot;run distro_bootcmd; &quot; \ &quot;run bootflash; &quot; \ &quot;setenv serverip 172.20.94.226; &quot; \ &quot;tftp 0x60003000 uImage; &quot; \ &quot;tftp 0x60500000 vexpress-v2p-ca9.dtb; &quot; \ &quot;setenv bootargs &apos;init=/linuxrc root=/dev/mmcblk0 rw console=ttyAMA0&apos;; &quot; \ &quot;bootm 0x60003000 - 0x60500000; &quot; 其中ip地址换成你自己的即可。 编译源码1234export ARCH=armexport CROSS_COMPILE=arm-linux-gnueabi-make vexpress_ca9x4_defconfigmake -j8 转换linux系统镜像格式12cd tools./mkimage -n &apos;Cortex-A9&apos; -A arm -O linux -T kernel -C none -a 0x60003000 -e 0x60003000 -d /path/to/zImage /path/to/uImage 八、测试运行qemu直接启动命令行界面123456789qemu-system-arm \ -M vexpress-a9 \ # 指定硬件平台 -m 256M \ # 指定内存大小 -kernel /path/to/zImage \ # 指定内核镜像路径 -dtb /path/to/vexpress-v2p-ca9.dtb \ # 指定设备树路径 -sd /path/to/a9rootfs.ext3 \ # 指定文件系统镜像路径 -append &quot;root=/dev/mmcblk0 rw console=ttyAMA0&quot; \#设置启动参数 -nographic \ # 无图形化界面 -smp 4 # 指定4个启动核心数 运行结果 图形化界面123456789qemu-system-arm \ -M vexpress-a9 \ -m 256M \ -kernel /path/to/zImage \ -dtb /path/to/vexpress-v2p-ca9.dtb \ -sd /path/to/a9rootfs.ext3 \ -append &quot;root=/dev/mmcblk0 rw&quot; \ -serial stdio \ -smp 4 运行结果 通过u-boot启动123456qemu-system-arm \ -M vexpress-a9 \ -m 256M \ -kernel /path/to/u-boot \ -sd /path/to/a9rootfs.ext3 \ -nographic 九、附录1. 源代码实现十、参考链接1. 用QEMU搭建arm嵌入式开发环境2. 添加Qemu网络支持并用u-boot+tftp引导内核启动]]></content>
      <categories>
        <category>Linux学习笔记</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记 -- qemu仿真运行RT-Thread]]></title>
    <url>%2F2018%2F01%2F20%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-qemu%E4%BB%BF%E7%9C%9F%E8%BF%90%E8%A1%8CRT-Thread%2F</url>
    <content type="text"><![CDATA[一、背景知识RT-Thread是一款有中国开源社区主导开发的嵌入式实时操作系统，包含嵌入式实时系统相关的各个组件：TCP/IP协议栈、文件系统、POSIX接口、图形界面、USB等，其硬实时核心主要采用C代码编写，具备高度可定制性，此外RT-Thread采用基于优先级的抢占式多线程调度。详情见 RT-Thread。 二、工具准备下载并解压RT-Thread源码1234wget https://github.com/RT-Thread/rt-thread/archive/v3.0.0.tar.gzmv v3.0.0.tar.gz rt-thread-3.0.0.tar.gztar -xzvf packages/rt-thread-3.0.0.tar.gz 安装scons构建系统1sudo apt install -y scons 安装交叉编译工具链1sudo apt install -y gcc-arm-none-eabi 三、编译RT-Thread12cd bsp/qemu-vexpress-a9scons 其中vexpress-a9是硬件平台类型，与前一篇博客中linux的硬件平台类型对应。 四、测试运行1qemu-system-arm -M vexpress-a9 -kernel /path/to/rtthread.elf -nographic 运行结果 五、附录1. 源代码实现]]></content>
      <categories>
        <category>Linux学习笔记</category>
      </categories>
      <tags>
        <tag>RT-Thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记 -- 基于逻辑回归的叶子分类]]></title>
    <url>%2F2017%2F12%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E4%BA%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%8F%B6%E5%AD%90%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[一、简介这篇博客是针对 Kaggle 竞赛中 Leaf Classification 而写的，简单说就是一个叶子分类问题。主要涉及方法选择、特征工程、二次分类三个问题，记录下来或许对于类似的分类问题有借鉴作用，仅供参考。 二、方法选择首先需要选择一种恰当的方法进行分类，考虑到目前我的PC性能以及训练所需要的时间，我决定从常用的统计学习方法选择一种效果较好的。利用题目已经给出的训练集，通过初步筛选，简单调参，除去明显不适合的方法，初步选择以下8种方法作为候选方法，分别为： 逻辑回归 LR 线性判别分析 LDA 支持向量机 SVM 朴素贝叶斯 Bayes K近邻 KNN 随机森林 RF 决策树 DT 梯度提升 GB 下面基于已有的训练集对这8种方法进一步分析筛选。题目中给出的训练集总共990份，我将其中600份作为训练集，390份作为测试集，分别用这8种方法进行训练并预测，得到以下结果：由上面两张对比图可以看到，LR、LDA、SVM 三种方法准确率最高，LR、Bayes、KNN 三种方法对数损失最小，并且 LR 与其余方法之间有数量级的差距，综合考虑准确率和对数损失，我最终选择了 LR 方法，也就是逻辑回归。值得一提的是，对数损失可以一定程度上反应模型预测的置信程度，尽管 LR、LDA、SVM 三者准确率相当，但是 LR 预测结果的置信度比另外两个高很多。 三、特征工程采用逻辑回归方法在现有特征上可以得到的对数损失为 0.02987，这是 Kaggle 评测的成绩，显然还需要进一步提取特征。特征工程应该是叶子分类中最重要的过程，我主要从以下三个方面进行。 基本特征这是7个类别的叶子形状，如下图：由图可见，不同类别的叶子在除去形状不同外，宽度和高度也有明显区别，据此我提取了以下五个特征： 宽度 width 高度 height 宽高比 width / height 宽高积 width * height 横纵方向 int(width &gt; height) 轮廓特征进一步，还可以利用 opencv 提取叶子的轮廓特征，关于 opencv 的具体用法看 这里 。我主要提取了以下特征： image moments area and perimeter centroid point convexity convex hull contour approximation Image moment 用维基百科的解释就是图片像素灰度的加权平均或者多个moment的函数值，Image moments 可以描述图片的一些特殊属性，如面积、中心点等。 Convexity 就是图像的凸边性，通俗的说就是图像的形状是凹的还是凸的。 Contour approximation 就是用一个多边形去近似图像轮廓，显然如果近似得越好，多边形的顶点数通常就越多，见下图： Convex hull 类似于 Contour approximation，不过是用凸包来包围图像轮廓，见下图：Convex hull 可以提供的特征信息有两个，凸包顶点，轮廓的最低点到凸包的距离，由于不同叶子其凸包具有不同的顶点数，无法直接利用其顶点坐标信息，我这里从 Convex hull 提取了两个特征，凸包顶点个数以及轮廓最低点到凸包的距离之和。图像中心点直接利用坐标即可，moments共24个值，因此从轮廓特征总共提取32个特征。 像素特征最后一步，图片的像素信息也是可以提取特征的，这里需要利用 PCA 对图片的像素信息进行降维。最终我保留了15维的信息，也就是提取了15个特征。 通过以上3步，我总共提取了52个新的特征，加上题目原本给出的192个特征，总计244个。然后，利用逻辑回归方法重新训练模型并预测，Kaggle 评测给出的对数损失为 0.00581，这个效果在实际应用中已经足够好了。 四、二次分类为了进一步提高成绩，我对预测结果进行了更详细地分析，发现以下信息：这是分类器对id为1304的测试用例做出的预测结果，可以发现最大预测概率也不超过0.5，如果将最大概率对应的类别作为最终的类别，其置信程度显然是不够的，实际上在这里就是错误的。造成这个结果的原因可能是某些类别的叶子相似度太高，以致于分类器难以准确划分。因此，可以考虑对这些“不好”的结果进行二次分类，这里有三个问题，如何衡量预测结果是不好的？用哪些数据作为二次分类的训练数据？二次分类用什么方法训练模型呢？ 针对问题一，我是这样考虑的。如果分类器对某一个测试用例的预测结果中，最大预测概率高于0.9或者预测概率高于0.05的类别数少于2，那么将最大预测概率对应的类别作为最终结果是可信的，反之则需要进行二次分类。通过筛选，在所有594个测试用例中，有4个用例是需要进行二次分类的，也就是说其余用例的最大预测概率均在0.9以上。 针对问题二，前一步我们已经得到了需要进行二次分类的测试用例，进一步，针对每一个测试用例找到预测概率大于0.05的类别，将这些类别对应的训练集数据作为二次分类的训练数据。统计之后，一共10个类别，总计100条训练数据。 针对问题三，用什么模型，我尝试过了上面提到的 Bayes、LDA、SVM、RF 等方法，得到的结果均不理想，最后我发现还是逻辑回归效果最好，不仅能够正确进行分类，还大大提高了最大预测概率。 下面是这10个类别对应叶子，汇总如下：通过二次分类，得到最终的结果，测试用例总计594个，其中585个测试用例最大预测概率高于0.99，593个高于0.94，只有id为1304的用例最大预测概率为0.81。为了提高竞赛成绩，我这里使用了一个小tricky，将所有预测结果中，最大预测概率对应的类别概率置为1，其余置为0。Kaggle 评测给出最后的对数损失值为0.0000，也就是说分类器的准确率为100%。 五、附录1.代码实现六、参考1.Leaf Classification 第一名方案]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django学习笔记 -- Celery的简单使用]]></title>
    <url>%2F2017%2F12%2F12%2FDjango%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Celery%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、简介Celery 是 Python 开发的分布式队列处理器，可以异步、分布式地处理任务，也可以执行定时任务。我们可能在 Django 中执行一些比较耗时的任务，如发邮件，我们总不能等待邮件发送完毕了再响应页面吧，同时有一些任务是需要在后台执行的，如爬虫和服务器缓存更新，这时候 Celery 就是一个不错的选择。 二、配置在使用 Celery 的时候我们还需要选择一个中间件，用来保存队列记录、执行记录，作为对 Celery 执行过程的数据支持，可以选择 Django 本身、mongodb 等，这里我们使用的是 Redis。 Redis 安装首先在这里下载编译好的 Windows 环境下的 Redis ，然后安装并配置环境变量。启动 Redis 服务器，在终端输入以下命令：1redis-server Python 库安装在 python 环境下安装 celery、redis，命令如下：12pip install celerypip install redis 以上命令会自动安装相关的依赖包，我用的 celery 版本是 3.1.23，在 python34 和 python35 下运行良好。 Django 配置在 settings.py 所在目录下新建 celery.py 文件，代码如下：12345678910111213141516171819from __future__ import absolute_import, unicode_literals # 必须在最上面import osfrom celery import Celeryfrom django.conf import settings# 配置环境变量project_name = os.path.split(os.path.abspath('.'))[-1]project_settings = '%s.settings' % project_nameos.environ.setdefault('DJANGO_SETTINGS_MODULE', project_settings)# 实例化 Celeryapp = Celery(project_name)# 使用 Django 的 settings 文件配置 Celeryapp.config_from_object('django.conf:settings')# Celery 加载所有注册的应用app.autodiscover_tasks(lambda: settings.INSTALLED_APPS) 为了让项目能够加载这个文件，我们还需要在该目录下的 __init__.py 文件中添加如下代码：12from __future__ import absolute_import, unicode_literalsfrom .celery import app as celery_app #引入celery实例对象 最后在 settings.py 文件中配置 Celery，代码如下：12345678910111213# Celery 中间件 redis://redis 服务所在的 ip 地址:端口/数据库号BROKER_URL = 'redis://localhost:6379'# Celery 结果返回，可用于跟踪结果CELERY_RESULT_BACKEND = 'redis://localhost:6379'# Celery 内容等消息的格式设置CELERY_ACCEPT_CONTENT = ['application/json']CELERY_TASK_SERIALIZER = 'json'CELERY_RESULT_SERIALIZER = 'json'# Celery 时区设置，与 settings.py 文件的时区设置保持一致CELERY_TIMEZONE = 'Asia/Shanghai' 三、使用耗时任务在 Django 项目 app 下 models.py 文件所在目录下建立 tasks.py 文件，你可以将所有需要异步执行、后台执行的代码放在这里。这里摘取我 Django 项目中部分代码片段：12345from celery.task import task@task(name="weibo_spider_friend")def weibo_spider_friend(username, friend): # some spider operations 函数名上面是 celery 的装饰器 task，name参数是任务的名字，当然还有其他方法，请自行查阅。需要注意的是，这个函数的参数必须是可 JSON 序列化的数据，这是因为之前我们在 settings.py 文件中指定了消息内容的类型。然后我们在 views.py 或者 views 文件夹下的视图函数中调用准备好的函数，代码如下： 1234567from ..tasks import weibo_spider_frienddef add_found_friend_weibo(request): # some code args = &#123;'id': friend.id, 'weibo_ID': friend.weibo_ID&#125; weibo_spider_friend.delay(request.user.username, args) # some code 当我们调用这个视图函数的时候就会执行这个任务，在此之前，还有最后一个操作，在确保 Redis 已经启动并且可用的前提下，打开终端输入以下命令：1celery -A django_project_name worker -l info 注意是在项目根目录下输入这条指令。运行效果如下，其中 tasks 下是我项目的所有任务集： 定时任务同样，和前面一样我们需要在 tasks.py 下准备好需要定时执行的函数：123456from celery.task import periodic_taskfrom celery.schedules import crontab@periodic_task(run_every=(crontab(minute='*/5')), name="weibo_spider")def weibo_spider(): # some code 异步任务装饰器参数，name 是任务名字，run_every 定义任务执行时间间隔，具体用法请参考 这篇博客。分别在两个终端窗口执行以下命令：12celery -A django_project_name worker -l infocelery -A django_project_name beat -l info 第二条命令的执行效果如下，下面发出的三条命令是我项目中三个定时任务： 四、Supervisor &amp;&amp; Celery详细请参考 这篇博客，这是 Linux 下的使用姿势，Windows 下我实在不知呀，感受到了 Windows 对于程序员深深的恶意，正在弃坑中。 五、参考1.Django 初步使用 Celery2.Django Celery 定时任务和时间设置3.服务器使用 Supervisor 后台运行 Celery]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记 -- 基于libsvm的中文文本分类]]></title>
    <url>%2F2017%2F12%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8BSVM%2F</url>
    <content type="text"><![CDATA[一、简介SVM 是 Cortes 和 Vapnik 于1995年首先提出的，它在解决小样本 、非线性及高维模式识别问题中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。支持向量机方法是建立在统计学习理论的 VC 维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性和学习能力之间寻求最佳折衷，以期获得最好的泛化能力。 libsvm 是台湾大学林智仁博士等开发设计的一个操作简单、易于使用、快速有效的通用 SVM 软件包，可以解决分类问题(C−SVC、ν−SVC)，回归问题(ε−SVR、v−SVR)以及分布估计(one−class-SVM)等问题，提供了线性、多项式、径向基和 S 形函数四种常用的核函数供选择，可以有效地解决多类问题、交叉验证选择参数、对不平衡样本加权、多类问题的概率估计等。libsvm 是一个开源的软件包，不仅提供了 libsvm 的 C++语言的算法源代码，还提供了 Python、Java、R、MATLAB、Perl、Ruby、LabVIEW以及 C#.net 等各种语言的接口，可以方便的在 Windows 或 UNIX 平台下使用，也便于科研工作者根据自己的需要进行改进。 二、预处理1.语料准备我是使用搜狗实验室的 新闻数据 作为学习语料的，这份语料来自搜狐新闻2012年6月—7月期间国内、国际、体育、社会、娱乐等18个频道的新闻数据，提供URL和正文信息，适用于文本分类、事件检测跟踪、新词发现、命令实体识别等任务的学习。 2.分词处理分词相关的算法不少，这也是一个可以深入研究的课题，而分词结果的好坏也直接影响后续任务的效果。如果自己没有现成的模型，可以使用公开的分词工具，如 结巴分词，该工具提供了Python、Java、C++等多种安装方式，可自行选择。Github 上有详细的教程，我这里不再重复演示。 3.特征提取特征提取是整个文本分类中最关键的一步。虽然 SVM 在高维模式识别上具有优势，但对于文本分类的词库来说，有十几万的词量是很常见的，我们需要把 对类别区分度高的词 挑选出来，简单来说，如果一个词语在大多数类别的文章中都频繁出现，那么这个词语对于类别是没有区分度的，相反，如果一个词语仅在少数类别的文本中出现，那么这个词语就是有用的。 特征选择的方式有很多，下面仅对卡方检验（chi）做简单介绍，卡方检验的核心思想就是 利用概率论中的χ²分布来衡量实际值与理论值的差异程度。举个例子，假设我们有一堆文本，需要判断文本包含某个词语（如吴亦凡）是否与该文本的类别（如娱乐)有关，我们通过简单统计可以得到下面的四格表： 组别 属于娱乐 不属于娱乐 合计 不包含吴亦凡 19 24 43 包含吴亦凡 34 10 44 合计 53 34 87 观察这个表格，直观上我们可以得到这样一个信息：包含吴亦凡的文本属于娱乐类别的比例高于对立面。下面我们首先假设 文本是否包含吴亦凡 与 文本是否属于娱乐类 是独立的，可以计算随机抽取一个文本，属于娱乐类别的概率是 $ （19 + 24） / （19 + 34 + 24 + 10） = 60.9 \% $。那么根据无关假设，我们可计算上述表格中各项数据的理论值，得到下面的四格表： 组别 属于娱乐 不属于娱乐 合计 不包含吴亦凡 $43 * 0.609 = 26.2$ $43 * 0.391 = 16.8$ 43 包含吴亦凡 $44 * 0.609 = 26.8$ $44 * 0.391 = 17.2$ 44 合计 53 34 87 下面我们可以通过卡方检验来验证区别其差异有无统计学意义，检验的基本公式如下 $ χ² = \sum\frac{(A - T)^2}{T} $ 或 $ χ² = \frac{(ad-bc)^2 n}{(a+b)(a+c)(b+d)(c+d)}$其中A为四格表中的实际值，T为对应的理论值，a、b为第一行两个数据，c、d为第二行两个数据。通过计算我们可以得到上述表格的χ²值为10.01。得到χ²值之后，我们可以通过查询卡方分布的临界值表（见附录）判断该值是否合理。自由度计算公式 V =（行数 -1）* （列数-1），显然上述四格表的自由度 V = 1。通过查表得到 P = 0.01 的临界值为 6.63，而10.01 &gt; 6.63，也就是说 文本是否包含吴亦凡 与 文本是否属于娱乐类 无关的可能性小于$1\% $，独立性假设不成立。 χ²分布的介绍就到这里，下面回到正题。对应到文本分类中，我们需要计算每个词的 a、b、c、d值： 在这个分类下包含这个词的文档数量 不在该分类下包含这个词的文档数量 在这个分类下不包含这个词的文档数量 不在该分类下且不包含这个词的文档数量 计算之后，我们可以得到每个分类下每个词的卡方分布值，除去 停用词 (对于分类没有帮助的词语)，每个类别取前1000个词（可根据实际情况调整）作为该类别的特征词，然后合并所有类别的特征词，去重，生成特征的唯一id。如下例： 1 逐项 2 深市 3 九寨沟 4 岛内 5 期望 … 注意，序号从1开始标注，这是libsvm的数据格式要求。 4.生成训练数据首先直接给出 libsvm 的数据格式：12lable1 index1:featureValue1 index2:featureValue2 index3:featureValue3 ... lable2 index1:featureValue1 index2:featureValue2 index3:featureValue3 ... 对应到文本分类上：1类别ID 特征序号1:特征值1 特征序号2:特征值2 特征序号3:特征值3... 其中特征序号必须是严格升序。那么，这里的特征值是什么呢？可以采用TF-IDF值，详细的介绍看 这里。这里仅给出计算公式： $ 词频(TF)= \frac{某个词在文章中的出现次数}{文章的总词数}$ $ 逆文档频率(IDF)= log（\frac{语料文本总数}{包含该词的文档数+1}）$ $ TF-IDF = 词频(TF)* 逆文档频率(IDF) $ 这样，我们就将文本数据转换为了可训练的 libsvm 数据格式，下面给出一个数据示例：12+1 1:0.708333 2:1 3:1 4:-0.320755 5:-0.105023 6:-1 7:1 8:-0.419847 9:-1 10:-0.225806 12:1 13:-1 -1 1:0.583333 2:-1 3:0.333333 4:-0.603774 5:1 6:-1 7:1 8:0.358779 9:-1 10:-0.483871 12:-1 13:1 三、libsvm 安装这里主要讲一下 windows 下 python 版本的 libsvm 安装，环境如下： Windows 10 64位 Python 3.5 64位 Visual C++ 14 首先到官网下载 libsvm 源码，windows目录下已经有编译好的32位动态链接库，如下图 但如果你使用的是 64位 python，就需要自己进行编译了。windows下编译工具 Visual C++ Tools 2015 ，对应 python 3.5。 打开终端 VS2015 x86 x64 Cross Tools Command Prompt ，进入到下载好的 libsvm 根目录，输入命令： nmake -f Makefile.win clean all 编译之后，得到下图，注意日期： 剩下的步骤很简单了，首先将动态链接文件 libsvm.dll 拷贝到系统目录，C:\windows\system32\，然后将python目录下 svm.py 和 svmutil.py 拷贝到你的 python 包目录下。好的，大功告成。 注意，如果你无法编译64位动态链接文件，这里 我已经编译好了，亲自验证 python3.4 和 python3.5 均可以使用，虽然 python3.4 对应的编译环境是 Visual C++ 10。 四、libsvm 使用示例12345678910111213141516171819202122232425262728def train(data_path, model_path, param_str=''): """ function: train model and save it into file data_path: path of training data model_path: file path to save the model param_str: parameter string for training model """ if not param_str: param_str = '-c 200 -g 9.0e-6 -q' y, x = svm_read_problem(data_path) svm_prob = svm_problem(y, x) svm_param = svm_parameter(param_str) model = svm_train(svm_prob, svm_param) svm_save_model(model_path, model)def predict(data_path, modal_path): """ predict label for unlabel data data_path: path of testing data model_path: file_path of the model """ y, x = svm_read_problem(data_path) svm_model = svm_load_model(modal_path) p_label, p_acc, p_val = svm_predict(y, x, svm_model) return p_label, p_acc, p_val 上面仅仅是一个代码示例，详细用法请参考文档说明，根目录和 python 目录下都有 README 文件。 此外，对 tools 目录下的 grid.py 文件用法进行简单说明，这个文件依赖于 svm-train.exe 文件，帮助我们更加快捷的找到最佳训练参数，python 环境下运行:1python grid.py training_file 输出结果中最后一行即为最佳参数，当然你可以根据实际情况对 grid.py 代码进行修改。 另外，windows目录下有一个 svm-scale.exe 可执行文件，用于规范化特征值到指定范围，如：12svm-scale -l 0 -u 10 -s range_file training_file &gt; training_scalesvm-scale -r range_file testing_file &gt; testing_scale 上述两条命令将数据的特征值规范到 0-10 之间，-l、-u 参数是特征值上下界，-s range_file 是将范围存到文件中，便于规范测试数据时使用，-r range_file 即读取范围文件。更详细的用法请查看 libsvm 根目录下 README 文件。 五、附录1. 相关代码实现 2. 卡方分布临界值表 六、参考链接1.卡方检验原理及应用2.x2检验（chi-square test）或称卡方检验3.基于libsvm的中文文本分类原型4.TF-IDF与余弦相似性的应用]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>文本分类，SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序及其优化]]></title>
    <url>%2F2017%2F05%2F14%2F%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[一、算法思想快速排序算法是对冒泡排序的改进，在冒泡排序中，元素的比较和移动都是在相邻单元中进行的，因此可通过减少总的比较次数和移动次数，增大记录的比较和移动距离，从而提高算法性能。快速排序采用分治策略来减少排序过程中的比较次数，首先将原问题分解为若干个与原问题形似的子问题，然后递归地求解子问题，如果子问题的规模足够小，则直接求解子问题，最后将每一个子问题的解组合成原问题的解。 二、核心实现123456789101112def qsort(arr, stack): """called by main sorting function""" high, low = stack.pop(-1), stack.pop(-1) pivot, less_equal, greater = arr[high], [], [] for x in arr[low:high]: if x &lt;= pivot:less_equal.append(x) else:greater.append(x) arr[low: high + 1] = less_equal + [pivot] + greater if len(less_equal) &gt; 1: stack.append(low);stack.append(low + len(less_equal) - 1) if len(greater) &gt; 1: stack.append(high - len(greater) + 1);stack.append(high) 此函数是所有二路快排核心算法的python实现，供主函数调用，为了防止栈溢出，采用非递归方式实现。参数stack是一个全局变量，保存递归进程中每一个序列的首尾元素下标。此函数默认序列最右端元素为划分点，所以在调用函数之前，必须将划分点交换到序列最右端。 三、划分点选取1.选取最右端第一种选取方法最简单，直接选取序列左端或者右端的元素，在本例中初始化栈stack后便可直接调用上述函数。相关代码如下：123stack = [0, len(arr)-1]while stack: qsort(arr, stack) 2.随机选取快排算法中选取划分点的原则是尽可能使划分结果均匀分配，在方法1中我们直接选取序列最右端元素往往会得到一个坏的结果，特别当序列是有序的或者趋近于有序的时候，若采用递归方式实现，则会达到最大递归次数，显然效果是很差的。另外一种方案是随机选取序列中的一个元素作为划分点，这样可以尽可能避免极端情况的出现，实验证明其效果也是可观的。相关代码如果下：123456stack = [0, len(arr)-1]while stack: low, high = stack[-2], stack[-1] pos = random.randint(low, high) arr[pos], arr[high] = arr[high], arr[pos] qsort(arr, stack) 首先调用标准库随机数产生函数，返回一个序列范围内的元素下标，然后将此元素作为划分点交换到序列最右端，调用核心实现函数即可。 3.两数取异另外一种选取划分点的方法是，单向遍历序列元素，选取第一次遇到的两个不同元素中较大或者较小的一个作为划分点，如果序列所有元素均相等，则排序已完成。相关代码如下：1234567891011121314stack = [0, len(arr)-1]while stack: low, high = stack[-2], stack[-1] pos, k = low, low + 1 while k &lt;= high: if arr[k] &gt; arr[pos]: pos = k;break elif arr[k] &lt; arr[pos]: break k += 1 if k == high + 1: stack.pop(-1); stack.pop(-1); continuearr[pos], arr[high] = arr[high], arr[pos]qsort(arr, stack) 如图所示，设置指针k遍历序列，如果遇到一个不等元素则找到划分点，本例中选取两个不同元素中较大者，如果遍历完序列，则该序列已经有序。 4.三数取中考虑我们选取划分点的原则，是尽可能使划分结果均匀，那么我们需要尽可能选取序列的中值作为划分点，而三数取中法可以帮助我们以更大的概率选择到序列的中值，具体方法选取序列最左端、最右端、中间位置三个元素的中位数作为划分点。相关代码如下：1234567if mid == -1: mid = (low + high) // 2if arr[mid] &lt;= arr[low] &lt;= arr[high] or arr[high] &lt;= arr[low] &lt;= arr[mid]: return lowif arr[low] &lt;= arr[mid] &lt;= arr[high] or arr[high] &lt;= arr[mid] &lt;= arr[low]: return midif arr[low] &lt;= arr[high] &lt;= arr[mid] or arr[mid] &lt;= arr[high] &lt;= arr[low]: return high 该段代码返回三数中位数的下标给调用者。我们只给出选取划分点的代码，主函数和方法二类似，详细代码见附录。 5.九数取中三数取中的方法在元素个数不是很大的时候效果比较好，实验证明当序列元素个数在7-40之间时效果较好，而当序列长度大于40时，效果下降，由划分点选取原则我们很容易想到扩大选数范围，则可以更大概率的选到中间值，具体方法是将序列分为8个相等部分，则有两个端点和七个中间点， 三个一组，分别选出中位数，然后选出中位数的中位数作为划分点。相关代码如下：12345size = high - low + 1median_1 = median_of_three(arr, low, low + size // 3)median_2 = median_of_three(arr, low + size // 3 + 1, high - size // 3)median_3 = median_of_three(arr, high - size // 3 + 1, high)return median_of_three(arr, median_1, median_2, median_3) 当元素序列小于7时，可以直接采取简单排序方法，如插入排序。 四、划分方法1.前后指针法前后指针法是通过单向遍历序列对序列元素进行划分操作，以从左向右遍历为例，首先设置两个指 针，第一个指向最左端元素的前一个位置，作为目的指针，另一个指向最左端元素，作为遍历指针。用第二个指针遍历序列元素，如果当前元素大于划分点，则递增遍历指针，否则首先递增目的指针，交换二者指向的元素，然后递增遍历指针，继续遍历序列剩余元素，直到划分完成。相关代码实现(C++)如下：123456789101112131415int partition(vector&lt;int&gt; &amp;arr, int low, int high)&#123; int pivot = arr[high]; int distionation = low; for(int i = low; i &lt; high; i++) &#123; if(arr[i] &lt; pivot) &#123; swap(arr, distionation, i); distionation++; &#125; &#125; swap(arr, distionation, high); return distionation;&#125; 2.左右指针交换法左右指针交换法同样设置两个指针，不过其使用方法不同，用两个指针分别从序列的两端开始遍历， 左指针遇到大于划分点的元素时停下，右指针遇到小于划分点的元素时停下，交换两个指针指向的元素，然后继续进行遍历，直到两个指针相遇，划分结束。相关代码实现(C++)如下：1234567891011121314int partition(vector&lt;int&gt; &amp;arr, int low, int high)&#123; int pivot = arr[low]; int p = low + 1, q = high; while(p &lt;= q) &#123; while(arr[p] &lt; pivot &amp;&amp; p &lt;= q) p++; while(arr[q] &gt;= pivot &amp;&amp; p &lt;= q) q--; if(p &lt; q) swap(arr, p, q); &#125; swap(arr, low, q); return q;&#125; 3.左右指针赋值法在方法2中，每次都是交换两个指针的内容，交换操作相当于三个赋值操作，效率较低，而左右指针赋值法即对此进行的改进，又称为填坑法。两个指针分别从序列两端交替进行遍历，首先取出最右端元素作为划分点，最右端位置空出，即为右坑。左指针向右遍历遇到大于划分点的元素时停下，将指针指向的元素值赋给右坑，则当前位置空出，即为左坑，然后右端指针向左遍历遇到小于划分点的元素时停下，同样赋值给左坑，交替进行，直到两个指针相遇。相关代码实现(C++)如下：12345678910111213int partition(vector&lt;int&gt; &amp;arr, int low, int high)&#123; int p = arr[low]; while(low &lt; high) &#123; while(arr[high] &gt;= p &amp;&amp; low &lt; high) high--; arr[low] = arr[high]; while(arr[low] &lt; p &amp;&amp; low &lt; high) low++; arr[high] = arr[low]; &#125; arr[low] = p; return low;&#125; 4.三路快排当序列中有很多相等元素，特别是当这些元素相邻时，可以对上述方法进行改进，即三路快排序，即将元素划分为三个部分，分别小于、等于、大于划分点元素，然后对小于、大于划分点元素的两个序列递归进行排序，而等于部分则已经完成排序。相关代码实现(python)如下：12345678910111213stack = [0, len(arr)-1]while stack: high, low = stack.pop(-1), stack.pop(-1) pivot, less, equal, greater =arr[random.randint(low, high)], [], [], [] for x in arr[low: high + 1]: if x &lt; pivot: less.append(x) elif x == pivot: equal.append(x) else:greater.append(x) arr[low: high + 1] = less + equal + greater if len(less) &gt; 1: stack.append(low); stack.append(low + len(less) - 1) if len(greater) &gt; 1: stack.append(high - len(greater) + 1);stack.append(high) 五、性能对比首先，通过使用相等序列、正序序列、逆序序列、随机序列、部分重复序列对上述排序函数(不包括C++实现的函数)进行测试，测试集生成方式如下图：12345678910arrays = [[25] * RecordsNum, list(range(RecordsNum)), list(range(RecordsNum, 0, -1)), [random.randint(1, 10000000) for k in range(RecordsNum)], [random.randint(1, 100) for k in range(RecordsNum)]]names = ['same records', 'positive sequence', 'negative sequence', 'random sequence', 'repetited records'] 对运行结果进行分析，得到如下性能对比图：图中每个柱状图从左到右依次为选取右端元素、随机选取、两数取异、三数取中、九数取中、三路快排以及标准库排序函数，纵坐标是函数的相对运行时间，容易看出实验结果符合上述原理分析。然后，我们通过对每一个函数使用不同规模的数据集，测试其运行时间变化曲线，每个函数采集30个点进行模拟分析，数据集生成方式如下：1arrays = [[random.randint(1, 10000000) for k in range(standard * m)] for m in range(1, 91, 3)] 对运行结果进行分析，得到如下性能曲线图：我们使用的数据集大小从50000到4500000线性增加，容易看出函数运行时间大致呈线性变化。 六、附录1.划分点选取与三路快排 python实现2.划分方法 C++实现七、参考1.关于快速排序的四种写法2.《算法导论》第二版]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最短路径算法小结]]></title>
    <url>%2F2017%2F04%2F14%2F%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、单源最短路径算法1.Bellman-Ford算法算法概述Bellman-Ford算法解决的是一般情况下的的单源最短路径问题，边的权值可以为负，并且该算法可以判断有向图中是否有负环。给定有向图G=(V,E)和相应边的权重，同时指定源点，Bellman-Ford算法返回一个布尔值，以表明有向图中是否含有负环，如果没有，算法将给出最短路径及其权重。 算法思想假设有向图G有|V|个顶点，那么我们容易知道任意两点之间的最短路径最多有|V|-1边，也就是简单路径，如果大于|V|-1条边，必然有环，如果是正环，去掉后路径变短，如果是负环，最短路不存在，如果是零环，去掉后不变。Bellman-Ford算法的思想很简单，就是对每一条边更新|V|-1次，如果有向图中没有负环，那么总能得到正确结果。那又怎么判断是否有负环呢？在进行|V|-1次松弛操作之后，如果仍然有变化，则再执行一次循环，如果仍然有更新操作发生，则必然含有负环。原因在上一段已经提到，如果有向图中没有负环，得到正确结果至多需要|V|-1次操作。实际上，并不是一定需要|V|-1次松弛操作，如果在某一次操作之后，没有任何更新，则再进行循环也不会有更新了，即当前结果就是最短路径，退出函数即可。补充说明一下什么是松弛操作。假设边(u,v)是图中的一条边，w(u,v)是该边的权重，dist[u]、dist[v]分别代表当前时刻顶点u、v到源点的最短距离，如果dist[u] + w(u,v) &lt; dist[v]，则需要对边(u,v)进行松弛操作，也就是更新顶点v到源点的最短距离。听起来感觉怪怪的。 算法实现下面给出伪代码：1234567891011Bellman-Ford(G, w, src)&#123; Initialize single source; for i = 1 to |V| for each edge(u,v) ∈ G.E Relax(u, v, w); if no change return true; if i == |V| and change occurs return false;&#125; 复杂度分析算法第3行初始化操作所需时间为$O(|V|)$，第4-10行每一次循环需要时间$O(|E|)$，且需要|V|次循环，所以总的运行时间为$O(|V|*|E|)$。 2.SPFA算法算法概述SPFA算法其实是针对Bellman-Ford的队列改进版，应用范围和Bellman-Ford是一样的。 算法思想容易看出，Bellman-Ford算法中对边的更新并不总是有意义的，对于某一条具体的边，如果它的起始顶点在前一次更新中没有受到影响，那么它在这次循环中也不会更新，所以没有必要对每一条边进行更新。SPFA算法采用一个队列保存受到影响的顶点，初始时刻将源点入队，每一次循环取出队首顶点，找到所有与之相邻的顶点，更新最短路，并将这些受到影响的顶点入队。所谓的受到影响，也就是在某一次更新中，顶点到源点的最短路径发生变化。直到队列为空时，退出循环。接下来的问题是如何判断是否有负环？如果有负环，前面的更新操作将陷入死循环。由于对某一条具体的边我们最多需要进行|V|-1次松弛操作，每一次松弛操作，相应的顶点就会入队，我们可以记录每一个顶点入队的次数，如果存在某一个顶点入队次数达到|V|次，那么有向图中必然有负环，强制退出循环，返回错误信息。 算法实现下面给出伪代码：12345678910111213SPFA(G, w, src)&#123; Enqueued src and Initialize info; while queue is not empty u = dequeue the queue for each edge(u,v) ∈ G.E Relax(u, v, w); if change occurs enqueued v; if v enqueued for |V| times return false; return true;&#125; 复杂度分析算法复杂度同Bellman-Ford算法，$O(|V|*|E|)$。 3.ASP算法算法概述ASP算法是针对带权重的有向无环图提出的一种线性算法。在有向无环图中，即使存在负权值的边，但因为没有权重为负值的环，最短路径是存在的。 算法思想首先，该算法对有向无环图进行一次拓扑排序，以便确定结点之间的一个线性次序。拓扑排序其实和排序算法没有任何关系，那么它为什么又叫做排序呢？在拓扑序列中有这样一个性质，如果一条边有向边(u,v)在有向图G中，那么在有向图顶点的拓扑序列中，顶点u一定在v之前。具体原因，请参考拓扑排序算法原理。这样一来，我们只需要按照拓扑序列对结点进行一遍处理即可，因为一旦更新某一个顶点，则该顶点即被确定，不会再更新。在对有向图顶点进行拓扑排序的同时，也就判断出有向图中是否包含有环，因为拓扑排序是不能对有环图使用的。 算法实现下面给出伪代码：12345678ASP(G, w, r)&#123; topologically sort the vertices of G; Initialize single source; for each vertex u, taken in topologically sorted order for each vertex v ∈ G.Adj[u] Relax(u, v, w);&#125; 复杂度分析复杂度分析非常容易。算法第3行拓扑排序时间为$O(|V|+|E|)$，第4行为$O(|V|)$，第5-7行恰好对每条边松弛一次，即$O(|E|)$。那么总的算法时间为$O(|V|+|E|)$，是线性时间。 4.Dijkstra算法算法概述Dijkstra算法解决的是带权重的有向图上单源最短路径问题，该算法要求所有边的权重都为非负值。采用合适的方法实现，Dijkstra算法的运行时间要低于Bellman-Ford算法的运行时间。 算法思想Dijkstra算法其实是一种贪心算法。该算法将有向图的顶点分为两个集合，一个是S，包含所有已经确定到源点最短路径的顶点，另一个是V-S，没有确定的顶点集合。算法重复从结点集V-S中选出最短路径估计最小的顶点u加入集合S，然后对所有从u出发的边进行松弛操作，直到S = V。为什么该算法中不能有负权值的边呢？还要强调的一点是，在Djkstra算法中一旦某个顶点已经加入集合S，则在后续更新中不再考虑该顶点。下面举例说明权值为负会出现什么状况：在上图中，源点为a，根据Dijkstra算法首先会选择顶点b，然后才是顶点c，但此时顶点b已经加入集合S，无法再进行更新，那么对于顶点b我们将得到最短路径为5，显然a-&gt;c-&gt;b才是最短路径，权值和为4。 算法实现我们可以采用优先队列来维持集合V-S，下面给出算法伪代码：1234567891011Dijkstra(G, w, src)&#123; Initialize single source; S = ∅; Q = G.V; while Q != ∅ u = Delete-Min(Q); S = S ∪ &#123;u&#125;; for each vertex v ∈ G.Adj[u] Relax(u, v, w);&#125; 复杂度分析算法第7行每次Delete-Min操作需要时间$O(1)$，对于每一个顶点只出队一次，算法第9-10隐藏一个Decrease-Key操作，该操作时间为$O(lg|V|)$，一共|E|条边，则总运行时间为$O(|E|*lg|V|)$。 二、全源最短路径算法1.基于矩阵乘法的动态规划算法算法概述基于矩阵乘法的动态规划算法是用来解决所有结点对最短路径问题的。首先，我们需要知道这样一条引理：一条最短路径的所有子路径都是最短路径。引理的证明请参考《算法导论》引理24.1。考虑从结点u到结点v的一条最短路径p，假定p至多包含m条边，并且有向图中没有负环。如果 u = v，则p的权重为0且不包含任何边。如果u != v，则将路径分解为u~k-&gt;v，其中路径p’:u~k至多包含m-1边，且p’为结点u到k的一条最短路径。因此dist[u][v] = dist[u][k] + w(k,v)。 算法思想设L(m)[u][v]为从结点u到结点v的至多包含m条边的任意路径中的最小权重，对于需要计算的L(m)[u][v]是L(m-1)[u][v]的最小值和从u到v的最多由m条边组成的任意路径的最小权重，我们通过检查j的所有可能前驱来获得该值，也就是说: L(m)[u][v] = min { L(m)[u][v], min { L(m-1)[u][k] + w(k,v) }}， 0 &lt;= k &lt;= |V|-1 那么又怎样判断图中是否有负环呢？简单分析可以得出，有向图中有负环当前仅当存在顶点 i，使得 L(m)[i][i] &lt; 0，证明略。此方法介绍比较简略，详细请参考《算法导论》25.1。 算法实现边拓展：1234567891011EXTEND_SHORTEST_PATHS(L, W) &#123; n = L.rows; let L' = l'(i,j) be a new n*n matrix; for i = 1 to n for j = 1 to n l'(i,j) = ∞ for k = 1 to n l(i,j) = min &#123;l(i,j)', l(i,k) + w(k,j)&#125; return L';&#125; 主程序：123456789SLOW-ALL-PATHS-SHORTEST-PATHS(W) &#123; n = W.rows; L[1] = W; for m = 2 to n-1 let L[m] be a new n*n matrix; L[m] = EXTEND_SHORTEST_PATHS(L(m-1), W); return L[n-1];&#125; 如果采用重复平方技巧:1234567891011SLOW-ALL-PATHS-SHORTEST-PATHS(W) &#123; n = W.rows; L[1] = W; m = 1; while m &lt; n - 1 let L[2m] be a new n*n matrix; L[2*m] = EXTEND_SHORTEST_PATHS(L(m), L(m)); let m = 2*m; return L[n-1];&#125; 复杂度分析显然边拓展需要的时间为$O(|V|^3)$，主程序的循环总共|V|次，则总运行时间为$O(|V|^4)$。如果采用重复平方技巧，主程序循环次数为lg|V|，则总运行时间降为$O(|V|^3 * lg|V|)$。 2.Floyd-Washall算法算法概述Floyd-Washall算法也是一种动态规划算法，是用来解决全源最短路径问题的。假设顶点u和v是有向图G中的任意两个顶点，如果u和v之前有有向边，则u和v之间有路径，但不一定是最短的，也许经过某些中间点之后会使路径长度更短。我们可以考虑系统地在原路径中间加入每个顶点，然后不断调整当前路径即可。 算法思想假设求顶点vi到顶点vj的最短路径。如果从 vi 到 vj 存在一条长度为L[i][j]的路径，该路径不一定是最短路径，尚需进行 n 次试探。首先考虑路径 (vi, v0, vj) 是否存在。如果存在，则比较 (vi, vj) 和(vi, v0, vj) 的路径长度，取长度较短者为从 vi 到 vj 的中间顶点的序号不大于0的最短路径。假设在路径上再增加一个顶点 v1，也就是说，如果 (vi ,…, v1)和(v1,…, vj) 分别是当前找到的中间顶点的序号不大于0的最短路径，那么(vi,…, v1 ,…, vj)就是有可能是从vi到vj的中间顶点的序号不大于1的最短路径。将它与已经得到的从 vi 到 vj 中间顶点序号不大于0的最短路径相比较，从中选出中间顶点的序号不大于1的最短路径，再增加一个顶点v2，继续进行试探。一般情况下，若 (vi ,…, vk) 和 (vk, …, vj) 分别是从 vi 到 vk 和从 vk 到 vj 的中间顶点序号不大于 k-1 的最短路径，则将 (vi ,…, vk ,…, vj)和已经得到的从 vi到 vj 且中间顶点序号不大于k-1的最短路径相比较，其长度较短者便是从 vi 到 vj 的中间顶点的序号不大于 k 的最短路径。下面是路径长路的迭代公式，且L(0)[i][j] = w(i,j): L(m)[i][j] = min { L(m-1)[i][j], L(m-1)[i][k] + L(k-1)[k][j] }，0 ≤ k ≤ |V|-1 同样，有向图中有负环当前仅当存在顶点 i，使得 L(m)[i][i] &lt; 0。下面举例说明，有这样一个有向图：初始矩阵：加入结点0后：加入结点1后：加入结点2后： 算法实现下面直接给出主要实现并做出相关分析：123456789101112for (int k = 0; k != size; ++k) for (int i = 0; i != size; ++i) for (int j = 0; j != size; ++j) if (dist[i][k] + dist[k][j] &lt; dist[i][j]) &#123; if (dist[i][k] == infinity || dist[k][j] == infinity) //negative weight edge. continue; dist[i][j] = dist[i][k] + dist[k][j]; nextV[i][j] = nextV[i][k]; if (i == j &amp;&amp; dist[i][j] &lt; 0) return false; //there is a negative weighted circuit. &#125; 上述代码中，dist[i][j]代表当前时刻结点i到结点j的路径距离，size是图的顶点个数，nextV是记录路径的数组，请读者自行分析其如何工作的。在程序第10行，对有向图中是否有负环进行了判断。 复杂度分析显然，Floyd-Washall算法的运行时间为$O(|V|^3)$。 3.Johnson算法算法概述Johnson算法是用于解决稀疏图全源最短路径问题的方法。Johnson算法的渐进表现要优于重复平方法和Floyd-Washall算法。Johnson实质上是对有向图每一个顶点调用Dijkstra算法，但是由于Dijkstra算法不能对负权重的边进行操作，调用Dijkstra算法之前需要将有向图转换为符合要求的新的有向图，这需要借助于Bellman-Ford算法。 算法思想具体的算法介绍这篇博客已经讲得非常清楚了，所以我就不再介绍了。 算法实现下面给出伪代码：12345678910111213141516171819JOHNSON(G, w) &#123; compute G', where G'.V = G.V∪&#123;s&#125;, G'.E = G.V∪&#123;(s,v):v∈G.V&#125;, and w(s,v)=0 for all v ∈ G.V if BELLMAN-FORD(G', w, s) == FALSE print "the input graph contain a negative-weigh cycle" else for each vertex v ∈ G'.V set h(v) to the value if δ(s,v) computed by the Bellman-Ford algorithm for each edge(u,v)∈G'.E w'(u,v) = w(u,v) + h(u) + h(v); let D=&#123;d(u,v)&#125; be a new n*n matrix; for each vertex u∈G'.V run DIJKSTRA(G,w',u) to compute δ'(u,v) for all δ∈G.V; for each vertex v∈G.V d(u,v) = δ'(u,v) + h(v) - h(u); return D;&#125; 复杂度分析如果采用二叉堆实现Dijkstra算法，总运行时间为 O(|V||E|lg|V|)，如果采用斐波那契堆来实现Dijkstra算法，则总运行时间为 $O(|V|^2*lg|V| + |V||E|)$。 三、附录1.源代码实现四、参考1.带权最短路算法分析2.所有结点对的最短路径3.算法导论 第二版]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>图论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态哈夫曼编码压缩]]></title>
    <url>%2F2017%2F04%2F01%2F%E5%8A%A8%E6%80%81%E5%93%88%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[一、算法提出由前一篇博客我们可以知道，使用基于静态Huffman编码树的压缩算法对符号流进行编码，必须进行两遍扫描。第一遍扫描统计被编码对象中符号出现的频率，并创建Huffman树，获取具有前缀性质的Huffman编码，第二遍扫描按照已获得的Huffman编码对输入符号进行编码。并且，在存储正文编码结果之前，必须将Huffman树的信息存入压缩文件，上文中我们采用的策略是保存Huffman编码。这样的做法，存在一些问题。第一，对于短小的符号流来说，加上Huffman编码信息之后，在尺寸上可能变大，这使得Huffman编码的应用受到限制。第二，静态Huffman编码压缩采用两遍扫描的方式，如果将这种方式用于网络通信中，必然会引起较大的延时，如果用于压缩中，额外的磁盘访问会降低该算法的压缩速度。第三，静态Huffman编码压缩自始至终都采用同一套编码，不能对符号流局部统计变化规律做出反应，压缩效率仍然有提升空间。针对以上问题，有人提出动态Huffman编码方案，又称自适应Huffman。这种方案不需要事先构造Huffman树，而是随着编码的进行，逐步构造Huffman树。同时，这种方法对符号的统计也是动态进行的，意味着能够对符号流局部统计变化规律做出及时的反应，所以说动态Huffman也是一种贪心算法。 二、FGK算法1、算法概述动态Huffman算法最初由Faller和Galler分别提出，Knuth在1985年改进了这个算法，所以这个标准的动态哈夫曼算法又称做FGK算法。1987年，Vitter提出了一种新的改进算法，并且证明FGK算法编码后的位数T满足：S - n + 1 &lt;= T &lt;= 2S + t -4n +2，而Vitter提出的改进算法编码后的位数T满足：S - n + 1 &lt;= T &lt;= 2S + t -2n +1，其中S是静态哈夫曼算法的编码位数，t是编码文本的长度，n是文本中不相同符号的个数。可见动态哈夫曼编码的最坏情况也不会达到最优化编码的二倍。下面介绍一个定义，一棵二叉树满足兄弟性质当且仅当其满足： 1、所有叶结点有非负权值，所有内部结点除根结点外都有两个儿子，并且父节点的权值是儿子结点权值之和。 2、对所有结点进行编号，父结点编号比儿子大，并且编号大的结点权值不小于编号小的结点。 最理想的情况是每一层结点编号从左到右、由下到上满足非递减顺序，但在FGK算法中不总是满足层序性质。实际上只要兄弟结点满足连续编号并且父节点编号大于儿子结点，就能够保证huffman树是正确的。Galler已经证明满足兄弟性质的二叉树即huffman树。上面我们已经提到动态Huffman算法是随着编码进行，逐步构造Huffman树，每读取第t+1个字符时，都是在前t个字符所构造的Huffman树的基础上去更新Huffman树。现在的问题是怎么更新Huffman树才能使其始终满足兄弟性质呢？为了标记一个新符号的出现并且快速添加新的结点，FGK算法设置了一个0-node，即权值为0的结点，我们把它叫做NYT（Not Yet Transmitted）结点。NYT结点编号始终是最小的。初始时刻，二叉树只有一个NYT结点。读取一个字符，如果字符不在Huffman树中，为原来的NYT结点分配两个儿子结点，左儿子作为新的NYT结点，右儿子存储新的符号（权值为1）。然后从旧的NYT结点开始，更新结点以及其所有祖先结点的权值。但是如果直接更新结点权值，会破坏兄弟性质。因此，在更新权值之前，需要进行一个交换操作，找到和当前结点权值相同且编号最大的结点，如果就是当前结点，则当前结点权值直接加1，如果不是，则与当前结点交换，交换后的结点作为当前结点，然后结点权值加1。注意，交换操作是交换以目标节点作为根结点的两棵子树，但是结点编号不变。然后更新当前结点的父结点，重复操作，直到当前结点为根结点。如果符号已经在Huffman树中，则直接更新，方法同上。 2、文件压缩编码过程中，每读取一个符号，如果它已经在Huffman树中，直接输出现有编码，然后更新Huffman树；如果不在树中，我们首先把NYT结点编码输出，标记遇到一个未出现过的新符号，紧接着把新符号的二进制编码输出，然后更新Huffman树。算法流程图如下： 下面以字符串abcddbb为例，展示每一步的具体操作：1、初始状态，仅有唯一的NYT结点，NYT结点权值为0： 2、输入符号a，NYT编码为空，则直接输出a的二进制编码0110 0001，然后新建NYT结点和符号结点，对51号结点进行加一操作： 3、输入符号b，NYT编码为0，输出编码0 0110 0010，然后新建NYT结点和符号结点，对49、51号结点进行加一操作： 4、输入符号c，NYT编码为00，输出编码00 0110 0011，然后新建NYT结点和符号结点，对47号结点进行加一操作，然后对49号结点进行加一操作，但是49号结点不是块内编号最大的结点，因此需要先与50号结点进行交换操作： 5、当前结点更新为50号结点，对50、51号结点进行加一操作： 6、输入符号d，NYT编码100，输出编码100 0110 0100，然后新建NYT结点和符号结点，对45号结点进行加一操作，然后对47号结点进行加一操作，但是47号结点不是块内编号最大的结点，因此需要先和49号结点进行交换操作： 7、当前结点更新为49号结点，对49、51号结点进行加一操作： 8、输入符号d，已经存在，输出编码001，对44号结点进行加一操作，同样需要先与48号结点进行交换操作： 9、更新当前结点为48号结点，对48、50、51号结点进行加一操作： 10、输入符号b，已经存在，输出编码001，交换44号结点与47号结点： 11、对47、50、51号结点进行加一操作： 12、输入符号b，已经存在，输出编码10，交换47、49号结点： 13、对49、51号结点进行加一操作，至此结束。 至于编码串处理和文件写入操作，同静态Huffman压缩算法一样，也是八位一组，需要在文件头保留补0的个数。最后，在这里可以看到算法伪代码。 3、文件解压解压文件时，和编码时一样，动态更新Huffman树，遇到未出现过的字符，则直接输出，同时新建NYT结点和符号结点，更新Huffman树，如果符号已经出现，则直接进行更新操作。由上面的算法流程可知，动态Huffman编码压缩时，无需保存Huffman树的信息，只需要保证编码解码时使用相同的规则即可。此外，由于Huffman树是动态更新的，即使同样的字符，其Huffman编码也不一定相同。 三、Vitter算法1、算法概述Vitter算法，又称Algorithm V，是Vitter于1987年发表的（详见J. S. Vitter, “Design and Analysis of Dynamic Huffman Codes,” Journal of the Association for Computing Machinery, Vol. 34, No. 4, October 1987, pp. 825-845.）。尽管Vitter算法并没有提高FGK算法的时间复杂度，但是完全不同的更新方式，使得Huffman编码长度缩短，由上面的介绍可知Vitter算法的下界是优于FGK算法的。同时，为了使该压缩算法在实际压缩过程中能够在线性时间内完成，Vitter提出了一种数据结构，叫做”float tree”，但在附录的源码实现中没有采用此方式。Vitter提出的算法除了满足兄弟性质意外，还必须满足一下两点： 1、Huffman树中的结点编号必须满足从左到右、由下到上是非递减顺序。 2、对于给定权值，叶子节点所在块在内部结点所在块之前，即如果编号由大到小分配，内部结点块编号大于叶子节点编号。 说明：我们很容易知道对于满足性质1的Huffman树，相同权值的结点编号一定是连续的，请读者自行体会。作者Vitter在它的论文中是这样说的: One of the main features of Algorithm A is its use of implicit numbering in which the nodes in the Huffman tree are numbered in increasing order from bottom to top and at each level in increasing order from left to right. Another main feature is the invariant that all leaves of a given weight precede in the implicit numbering all internal nodes of the same weight. These two features are shown in [3] to guarantee good coding efficiency. 由性质1、2，我们可以想象，Huffman树中的结点一定是这样的序列：LTLTLTLT，其中L代表叶结点块，T代表内部结点块，并且序列的权值满足非递减顺序。注意，在FGK算法中我们提到了块是权值相等的结点集合，而在Vitter算法中，又分为了叶结点块和内部结点块，这是为了满足性质2，请读者注意区分。由此，Vitter完全抛弃了FGK算法中update策略，下面我们直接给出Vitter算法中update操作的伪代码： 123456789101112131415161718192021222324updateHuffmanTree(accept symbol)&#123; pos = pointer of symbol's node; if(pos == -1) &#123; /* a new symbol */ Create new NYT node and character node, and aissgn weight of the character node as 1; pos = pointer of the old NYT node; &#125; else &#123; /* already in the tree */ Swap pos in the tree with leader of its block; if(pos is the sibling of the NYT node) &#123; Increase weight of pos; pos = parent of pos; &#125; &#125; while(pos != root of the tree) SlideAndIncrement(pos) Increase weight of root node;&#125; 可见，在FGK算法中，更新主要依靠不断的交换操作来实现的，而在Vitter算法中，采用的是SlideAndIncrement操作。接下来给出SlideAndIncrement操作的伪代码: 12345678910111213SlideAndIncrement(accept node p)&#123; fp = parent of p; wt = p's weight; if(p is an internal node) Slide p in the tree higher than the leaf nodes of weight wt+1; else Slide p in the tree higher than the nodes of weight wt; p's weight++; if(p is internal node) return fp; return new parent of p;&#125; 由上面的伪代码可知，在FGK算法中，只需要找到块内最大的结点进行交换即可，而在Vitter算法中，需要找到所有满足条件的结点，然后将目标节点滑过指定的结点块。 2、文件压缩下面同样以字符串abcddbb为例，展示每一步的具体操作：1、初始状态,仅有唯一的NYT结点，NYT结点权重为0： 2、输入符号a，NYT编码为空，则直接输出a的二进制编码0110 0001，然后新建NYT结点和符号结点，对51号结点进行加一操作： 3、输入符号b，NYT编码为0，输出编码0 0110 0010，然后新建NYT结点和符号结点: 4、49号结点为内部结点，滑动到50号结点之前，对50号结点进行加一操作，原49号结点为内部结点，下一步从51号结点即根结点开始，根结点直接进行加一操作： 5、输入符号c，NYT编码为10，输出编码10 0110 0011，然后新建NYT结点和符号结点： 6、47号结点为内部结点，滑动到49号结点之前，对49号结点进行加一操作，原47号结点为内部结点，下一步从50号结点开始，无权值为2的叶结点，因此直接对50、51号结点进行加一操作： 7、输入符号d，NYT编码00，输出编码00 0110 0100，然后新建NYT结点和符号结点： 8、45号结点为内部结点，滑动到48号结点之前，对48号结点进行加一操作，原45号结点为内部结点，下一步从49号结点开始，无权值为1的叶结点，因此直接对49、51号结点进行加一操作： 9、输入符号d，已经存在，输出编码111，交换44、47号结点： 10、47号结点为叶结点，滑动到48号之前，对48进行加一操作，原47号是叶结点，下一步从50号结点开始，无权值为3的叶结点，故直接对50、51号结点进行加一操作： 11、输入符号b，已经存在，输出编码01，46号结点为叶结点且编号最大，无须交换，将46号结点滑动到47号结点之前，对47号结点进行加一操作，原46号结点为叶结点，下一步从当前结点的父亲即50号结点开始，对50、51号结点进行加一操作： 12、输入符号b，已经存在，输出编码10，47号结点为叶结点，交换47、48号结点，然后将48号结点滑动到49号结点之前，对49、51号结点进行加一操作，至此结束。 最后，在这里可以看到算法伪代码。同时，作者给出一个例子，体现出Vitter算法的确是使Huffman树变得更加平衡，缩短了Huffman编码。（其中有一点错误，输入第20个字符后，两种算法得到Huffman树并不完全相同，只是树的高度相等，请读者自行理解。） 3、文件解压同Vitter算法，略。 四、附录1、FGK算法源码实现2、Vitter算法源码实现五、参考1、Quake3 自适应哈夫曼2、Adaptive Huffman coding - FGK3、Adaptive Huffman coding - Vitter’s algorithm (Λ)4、Faller, Gallager, Knuth = Algorithm FGK5、Dynamic Huffman Coding 2 - Algorithm Vitter]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>文件压缩算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[静态哈夫曼编码压缩]]></title>
    <url>%2F2017%2F03%2F31%2F%E5%93%88%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[一、基本术语 WPL：树的带权路径长度，规定为所有叶子结点的带权路径长度之和。 哈夫曼树：给定n个权值作为n个叶子结点，构造的一棵具有最小WPL值的二叉树，又称最优二叉树。 编码：将一组对象(如字符集)中的每个对象用唯一的一个二进制位串表示，如ASCII、指令系统。 编码前缀性：如果一组编码中任意一个编码都不是其它任何一个码的前缀 ，则称这种编码具有前缀性， 简称前缀码。 平均编码长度：设每个字符Ci出现的概率为Pi，其二进制编码长度为Li，则$\Sigma Li*Pi$表示该组字符的平均编码长度。 哈夫曼编码：根据字符出现频率构造的具有最小平均编码长度的编码，又称霍夫曼编码，哈夫曼编码是不等长编码，且具有前缀性。 二、二叉哈夫曼编码压缩1、哈夫曼树的构造举例说明，比如我们要压缩这样一个字符串： beep boop beer! 首先建立一个字符到次数的映射表，如下图： 字符 ‘b’ ‘e’ ‘p’ ‘ ‘ ‘o’ ‘r’ ‘!’ 次数 3 4 2 2 2 1 １ 然后，把这些数据放到一个优先队列中，数据项按照权重排序，得到这样一个初始序列： 接下来，就是把这个优先队列转换成二叉树，我们始终从优先队列中取出头部的两个元素来构造一棵二叉树，第一个元素作为左结点，第二个作为右结点，并把这两个结点权重之和作为根节点的权重，然后把这颗二叉树放回优先队列中，得到如下图表： 同样，再把前两个取出来，形成一个权重为4的结点，然后放回优先队列中： 继续算法，得到下图： 最终，得到这样一棵二叉树： 此时，把二叉树路径上的左支编码为0，右支编码为1，得到下图： 先序遍历二叉树，我们可以得到每一个字符对应的哈夫曼编码，如下表： 字符 ‘b’ ‘e’ ‘p’ ‘ ‘ ‘o’ ‘r’ ‘!’ 编码 00 11 101 011 010 1000 1001 由上可见，哈夫曼编码是不等长编码，并且具有前缀性，最后我们可以得到Huffman编码： 0011 1110 1011 0001 0010 1010 1100 1111 1000 1001 2、文件压缩假设我们现在的文本是从文件中读取出来的，根据上述原理我们已经获得哈夫曼编码，下一步就是进行编码并写入压缩文件。编码很容易，再次读取文件，将每一个字符转换成对应的Huffman编码，我们可以得到一串01编码，当然不可能直接将这串二进制编码写入压缩文件，在这里，我采取的策略是将8位二进制编码转成对应的ASCII编码，然后把对应的字符写入压缩文件。但是这样做可能会遇到一个问题，编码得到的二进制串的大小不一定被8整除，则最后会余下少于8位的二进制编码串。此时，我们考虑在编码串最后补0，凑足8位，从而可以转换为一个字符。这样我们的确是完成了压缩，可是问题又来了，我们如何去解压这个压缩文件呢？首先，我们在编码串最后补0，解码的时候怎么知道压缩时是否补0或者补了多少个0呢？其次，对于不同的文章，字符出现的次数不一样，得到的哈夫曼树就不一样，因此哈夫曼编码也不一样，我们又按照什么规则去解压呢？针对以上问题，我采用了这样的策略，首先在压缩文件头部用一个字节保存压缩时补0的个数，然后将字符及其对应的哈夫曼编码按一定规则写入文件（即一个字符后面紧跟着其哈夫曼编码），写编码同样按照8位一个字节存储对应字符，然后再将压缩后的内容写入文件。值得注意的是，哈夫曼编码不等长，所以在写入编码之前，需要以最长编码所需要的字节数为准，为每一个编码分配相同大小的字节数，对于哈夫曼码位数不足时，首先补1，剩余位补0。这样做的好处是，在解压时我们可以先读取字符，然后读取相应字节大小并转换为二进制编码，并除去每一个编码的补位，则得到字符对应的Huffman码。我们以第一部分内容为例，易知只需一个字节便可保存每一个编码，补位后得到下图： 字符 ‘b’ ‘e’ ‘p’ ‘ ‘ ‘o’ ‘r’ ‘!’ 编码 00100000 11100000 10110000 01110000 01010000 10001000 10011000 最后，我们也需要将编码字符的个数以及我们为每一个编码分配的字节数写入压缩文件，即我们需要留下三个字节来保留预处理信息，然后写入Huffman编码表，最后写入压缩文本。考虑上面所有问题，我们得到写入压缩文件字符对应的二进制编码串应该是这样： 00000000 00000111 000000001 01100010 00100000 01100101 11100000 01110000 10110000 00100000 01110000 01101111 01010000 01110010 10001000 00100001 10011000 00111110 10110001 00101010 11001111 10001001 虽然这里压缩后得到22个字符，看上去不但没有压缩作用，反而放大了，那是因为我们存入了Huffman编码，而且在实际压缩时，文本中字符数肯定远远不止这些。 3、文件解压根据压缩原理，我们很容易可以得到解压的方法，首先读出预处理信息，包括压缩文本补0个数、Huffman编码个数、储存每一个编码分配的字节数，然后读取字符及其对应的编码，经简单的恢复处理后得到Huffman编码。根据得到的Huffman编码，我们需要重建Huffman树，以便于译码时遍历哈夫曼树。方法很简单，建立一个二叉树，初始有一个根结点，用每一个Huffman编码去遍历二叉树，如果遍历到叶结点，编码仍未结束，根据编码添加结点，编码为0添加左儿子，反之则添加右儿子，遍历结束，将编码对应字符写到叶结点中。用所有的Huffman码去遍历，最后得到的二叉树即压缩时所建的哈夫曼树。得到哈夫曼树之后，就可以读取压缩文本，转换为对应的二进制编码串，进行译码。译码就是一个遍历二叉树的过程，从根节点出发，遇0则转到左子树，反之转到右子树，直到叶结点，将叶结点中字符写入解压文本，然后再从根结点出发，找下一个字符，直到二进制串结束。当然，我们应该事先将二进制串最后补的0去掉。至此，二叉哈夫曼编码压缩及解压介绍完了，可见由于我们是针对二进制进行编码解码，所以哈夫曼压缩原则上是适用于任意格式文件的，但实际应用中我们只用于压缩文本文件，因为对于图片、音乐、视频文件基本没有压缩作用。 三、K叉哈夫曼编码压缩1、哈夫曼树的构造相对于二叉哈夫曼压缩，K叉哈夫曼压缩只是将二进制编码变为K进制，建树时同样采用优先队列，只是每次取出队列前K个结点，建立一棵K叉树，根结点权重为所有子结点之和，然后将根结点放回优先队列中，重复合并，直到队列中只有一个结点，则得到K叉哈夫曼树。可是有一个问题？最后剩下的结点数可能小于K，出现这种情况怎么办呢？首先我们看一看什么情况下能够刚好建成一棵K叉树。假设有n个结点，每次取出K个放回1个，那么易得（K-1）|（n-1)时恰好建成一棵K叉树。也就是说，如果（n-1）%（k-1）！= 0，则需要加入（k-1）-（n-1）%（k-1）个虚结点，等价于第一次只合并（n-1）%（k-1）+ 1个结点。这样问题就得到了解决。举例说明，比如我们要压缩这样一个字符串： abeep boop ber! 同样，统计字符出现次数，得到下图： 然后，把这些数据放到一个优先队列中，数据项按照权重排序，得到这样一个初始序列： n = 8，取K = 3，则（n-1）%（K-1）== 1 ！= 0，则应添加一个虚节点，即第一次只合并2个结点，如下图： 继续合并： 最终，得到这样一棵3叉树： 此时，把3叉树路径上同一结点出发的路径从左到右依次编号0、1、2，得到下图： 先序遍历3叉树，我们可以得到每一个字符对应的哈夫曼编码，如下表： 字符 ‘b’ ‘e’ ‘p’ ‘ ‘ ‘a’ ‘o’ ‘r’ ‘!’ 编码 22 0 12 21 110 20 111 10 最后容易得到3叉Huffman编码： 110 22 0 0 12 21 22 20 20 12 21 22 0 111 10 2、文件压缩与二叉哈夫曼编码压缩不同的是，K叉哈夫曼编码压缩得到的是K进制串，不能直接写入压缩文件，需要先转换为二进制编码，即将每一位编码转换为二进制编码，并且所有转换后编码的位数应以最大编码为准。在上例中，采用的是三进制编码，故最大编码为2，需用2个二进制位表示，即0、1、2都应转换为2位二进制码，然后采用与二叉哈夫曼压缩相同策略写入压缩文件，此外K进制压缩预处理信息还应包括进制K。 3、文件解压与二叉哈夫曼类似，只是读取每一位K进制码时，都需要读取多位二进制码，然后转换得到。 4、压缩率当K=2时，即二叉哈夫曼压缩，取其余K值均比二叉更差，并且有如下变化规律： 由折线图可知，当K成指数增长时，压缩率最好，并且K = 2,4,8,16…依次减弱。这是因为K进制中编码小的也需要用相同大小的二进制位保存，浪费了空间。 四、附录1、二叉哈夫曼源码2、K叉哈夫曼源码五、参考1、HUFFMAN编码压缩算法]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>文件压缩算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++标准库容器deque的简单实现]]></title>
    <url>%2F2017%2F03%2F11%2FC%2B%2B%E6%A0%87%E5%87%86%E5%BA%93%E5%AE%B9%E5%99%A8%20%E2%80%94%E2%80%94%20Deque%2F</url>
    <content type="text"><![CDATA[一、deque简介deque是双向开口的连续性存储空间。虽说是连续性存储空间，但这种连续性只是表面上的，实际上它的内存是动态分配的，它在堆上分配了一块一块的动态储存区，每一块动态存储区本身是连续的，deque自身的机制把这一块一块的存储区虚拟地连在一起。它首次插入一个元素，默认会动态分配一块区域，用完之后再分配下一块动态存储区，然后虚拟地连在一起。deque的这种设计使得它具有比vector复杂得多的架构、算法和迭代器设计。它的性能损失比之vector，是几个数量级的差别。所以说，deque要慎用。 二、deque数据结构图示逻辑数据结构示意图： 实际数据结构示意图： 三、deque数据结构实现中控器在标准库中，deque先用一段小的连续空间即中控器，顺序存放了一个一个指针，然后这些顺序存放的指针再各自指向用来真正存放数据的连续性空间。当中控器使用率满载时，就会另觅一块更大的连续性空间，然后把指针一个一个复制过去，并销毁旧的空间。利用这种数据结构，deque就能方便地模拟自身的存储区是连续性空间的假象，并且可以实现双向插入删除的功能。我们来看一下示意图： 如图所示，map指向的连续空间存放各缓冲区指针，每一个指针指向一块存放数据的连续性空间。在本博客的实现中，采取的策略大同小异，只是用链表代替了中控器中的连续存储空间，链表中每一个结点保存所指向的连续性存储空间首指针和相邻缓冲区结点指针，便于前后移动寻找缓冲区元素，下面是中控器数据结构实现：1234567891011121314151617181920#define N 10struct map_node&#123; object *buffer; //连续性存储区域首指针 map_node *pre; //前一个相邻缓冲区结点指针 map_node *next; //后一个相邻缓冲区结点指针&#125;; class deque&#123; ......private: map_node *left = nullptr; //最左侧结点指针 map_node *right = nullptr; //最右侧结点指针 object *first = nullptr; //第一个元素指针 object *last = nullptr; //最后一个元素指针 size_t theSize = 0; //deque的大小 ......&#125; 如图所示，本文默认每一个缓冲区的大小为10个元素，在deque类的私有成员中包括了首尾缓冲区结点指针和首尾元素指针。 迭代器在标准库中，我们可以通过解引用的方式得到迭代器所指的值，为了能够从“deque内部结构”中得到中某个元素的确切位置，那么deque的迭代器应该具备怎么结构，我们可以考虑以下几点。首先，需要知道该元素位于哪个缓冲区的哪个位置；其次，一旦迭代器前进和后退有可能会跳跃至上一个或下一个缓冲区，为了判断跳跃的条件就需要知道，当前元素所在缓冲区的首尾指针。最后，如果前进或后退必须跳跃至下一个或上一个缓冲区。为了能够正确跳跃，deque必须随时掌握中控器的信息，通过中控器可以知道跳跃的缓冲区。所以，在迭代器中至少需要定义如下参数： 1、当前元素的指针2、当前元素所在缓冲区的首尾指针3、中控器中指向所在缓冲区的指针 我们再来看一看示意图： 下面是迭代器数据结构实现：12345678class iterator&#123; ...... object *elem = nullptr; //当前元素指针 object *buf = nullptr; //当前缓冲区首指针 map_node *nod = nullptr; //当前缓冲区在中控器中的结点指针 ......&#125; 四、附录1、Deque代码实现 五、参考资料1、C++ Primer,5th Edition2、深入剖析deque容器]]></content>
      <categories>
        <category>C++学习笔记</category>
      </categories>
      <tags>
        <tag>C++标准库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10种常见的排序方法总结]]></title>
    <url>%2F2017%2F02%2F25%2F10%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文简要总结了常见的10种排序算法，分别从算法思想、算法流程、算法性能、算法优化等方面进行了阐述。在这10种算法中，前面7种是基于比较的算法，后三种是非比较线性算法。本文所有排序除堆排序外，均默认为从小到大。此外，在文末附有我GitHub上的Python实现链接。 1.冒泡排序算法思想将待排序的记录看做是竖着排列的气泡,关键字较小的记录比较轻,从而要往上浮. 算法流程1、比较相邻的元素,如果下层元素小于上层元素,则交换;2、从底层向上,对每一对相邻元素重复步骤一,直到当前序列顶端;3、每一轮交换之后,序列元素除去顶端元素,对剩余序列元素重复以上操作;4、若当前序列只有一个元素,排序结束. 算法性能时间$O(n^2)$,空间$O(1)$ 算法稳定 算法优化记录某次遍历时最后发生数据交换的位置,这个位置之后的数据显然已经有序,不用再排序了.因此通过记录最后发生数据交换的位置就可以确定下次循环的范围,此位置初始化为待排序序列顶层元素位置. 代码实现1234567891011121314151617181920212223242526272829303132//原始代码：void bubbleSort(vector&lt;int&gt; &amp;arr)&#123; int n = arr.size(); for(int i=0; i != n-1; ++i) for(int j=n-1; j != i; --j) if(arr[j] &lt; arr[j-1]) &#123; int tmp = arr[j]; arr[j] = arr[j-1]; arr[j-1] = tmp; &#125;&#125;//改进后的冒泡排序: void bubbleSort_imp(vector&lt;int&gt; &amp;arr)&#123; int n = arr.size(); for(int i=0; i != n-1;) &#123; int next = n - 1; //标记最后一次数据交换的位置,初始化为顶层元素位置; for(int j=n-1; j != i; --j) if(arr[j] &lt; arr[j-1]) &#123; int tmp = arr[j]; arr[j] = arr[j-1]; arr[j-1] = tmp; next = j; &#125; i = next; //更新下一轮排序的序列范围; &#125;&#125; 2.快速排序思考改进快速排序算法是对冒泡排序的改进，在冒泡排序中，元素的比较和移动都是在相邻单元中进行的，因此可通过减少总的比较次数和移动次数，增大记录的比较和移动距离，提高算法性能. 算法思想分治策略 —— 减少排序过程中的比较次数1、分解:将原问题分解为若干个与原问题形似的子问题;2、求解:递归地求解子问题,若子问题规模足够小,则直接求解子问题;3、组合:将每一个子问题的解组合成原问题的解. 算法流程1、选取基准元素;2、划分:比基准元素小的放在左边,大于等于放在右边;3、对左右区间递归执行步骤1、2,直至各区间只有一个元素或者序列规模足够小. 基准元素选取方法1:取序列最左端元素;方法2:从序列arr[i],…,arr[j]中,将arr[i],arr[(i+j)/2],arr[j]排序,返回中值;方法3:从序列arr[i],…,arr[j]中,选择最先找到的两个不同关键字中的较大者.若无两个关键字不同,则已经有序. 算法性能时间$O(nlogn)$,空间$O(logn)$ 算法不稳定 算法优化1、若采用方法1,当初始序列是非递减序列时,快排性能下降到最坏情况$O(n^2)$,基准元素选取策略可采用方法2;2、在规模较小的情况下,采用直接插入排序. 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081//交换数组两个元素;void mySwap(int i, int j, vector&lt;int&gt; &amp;arr)&#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;&#125;//基准元素选取,方法3;int findPivot_1(int i, int j, const vector&lt;int&gt; &amp;arr)&#123; for(int k=i+1; k &lt;= j; ++k) if(arr[k] &gt; arr[i]) return k; else if(arr[k] &lt; arr[i]) return i; return -1; //排序已完成;&#125;//基准元素选取,方法2;int findPivot_2(int i, int j, vector&lt;int&gt; &amp;arr)&#123; int mid = (i + j) / 2; if(arr[i] &gt; arr[j]) mySwap(i, j, arr); if(arr[mid] &gt; arr[j]) mySwap(mid, j, arr); if(arr[i] &gt; arr[mid]) mySwap(i, mid, arr); return mid;&#125;//划分元素,小于放在左边,大于等于放在右边;int partition(int i, int j, int pivot, vector&lt;int&gt; &amp;arr)&#123; int left=i, right=j; while(left &lt;= right) &#123; while(arr[left] &lt; pivot)left++; while(arr[right] &gt;= pivot)right--; if(left &lt; right) &#123; int tmp = arr[right]; arr[right] = arr[left]; arr[left] = tmp; &#125; &#125; return left;&#125;void quickSort(int i, int j, vector&lt;int&gt; &amp;arr)&#123; int pivotPos = findPivot_1(i, j, arr); if(pivotPos != -1) &#123; int pivot = arr[pivotPos]; int k = partition(i, j, pivot, arr); quickSort(i, k-1, arr); quickSort(k, j, arr); &#125;&#125;//改进后的快速排序算法;void quickSort_imp(int i, int j, vector&lt;int&gt; &amp;arr)&#123; if(j - i &lt;= 5) insertSort(i, j, arr); //序列规模小,采用直接插入排序; else &#123; int pivot = arr[findPivot_2(i, j, arr)]; //选取基准元素; int k = partition(i, j, pivot, arr); //划分元素; quickSort_imp(i, k-1, arr); //对左右序列递归求解问题; quickSort_imp(k, j, arr); &#125;&#125;void quickSort(vector&lt;int&gt; &amp;arr)&#123; //quickSort(0,arr.size()-1,arr); quickSort_imp(0,arr.size()-1,arr);&#125; 3.插入排序算法思想依次选取一个待排序的元素,在已排序序列中从后向前扫描,找到相应位置并插入. 算法流程1、从第一个元素开始,该元素可认为已经排好序;2、取出待排序序列第一个元素,即已排序序列后一个元素;3、在已排序序列中从后往前扫,如果被扫描元素大于新元素,则将该元素后移一位;4、重复步骤3,直到找到一个小于等于目标值的元素,将元素插入在其后面;5、重复步骤2-4,直到待排序序列只有一个元素,则排序完成. 算法性能时间$O(n^2)$,空间$O(1)$ 算法稳定 算法优化直接插入排序每次寻找插入位置时,是按顺序依次往前找,可采用二分查找改进,即折半插入.折半插入的时间复杂度降为$O(nlogn)$; 代码实现12345678910111213141516171819202122232425262728293031323334//直接插入;void insertSort(vector&lt;int&gt; &amp;arr)&#123; for(int i=1; i!= arr.size(); ++i) &#123; int j = i; int tmp =arr[i]; for(; j !=0 &amp;&amp; tmp &lt; arr[j-1]; --j) arr[j] = arr[j-1]; arr[j] = tmp; &#125;&#125;//折半插入;void insertSort_imp(vector&lt;int&gt; &amp;arr)&#123; for(int i=1; i != arr.size(); ++i) if(arr[i] &lt; arr[i-1]) &#123; int tmp = arr[i]; int low = 0, high = i-1, mid; while(low &lt;= high) &#123; mid = (high + low) / 2; if(tmp &lt; arr[mid]) high = mid - 1; else low = mid + 1; &#125; for(int j = i; j != low; --j) arr[j] = arr[j-1]; arr[low] = tmp; &#125;&#125; 4.希尔排序思考改进希尔排序是对直接插入排序的改进,若排序序列基本有序时,直接插入排序的效率可以大大提高,同时由于直接插入排序算法简单,当待排序序列数量较小时效率也很高. 算法思想将整个待排序记录分割成若干个子序列,在子序列中分别进行直接插入排序,当整个序列中的记录基本有序时,对全体记录进行直接插入排序.如何分割子序列是希尔排序的关键,不同的分割策略,算法的效率亦不同. 算法流程1、选取合适的gap(&lt;n),将全部元素分割成gap个子序列,所有相距为gap个元素放入同一个子序列;2、对每一个子序列进行直接插入排序;3、缩小间隔gap,例如 gap=gap/2;4、重复步骤1-3,直到gap=1,将所有元素放入同一个序列中进行直接插入排序,排序结束. 算法性能时间$O(nlogn)$~$O(n^2)$,空间$O(1)$ 算法不稳定 代码实现123456789101112void shellSort(vector&lt;int&gt; &amp;arr)&#123; for(int d=arr.size()/2; d &gt;= 1; d /= 2) for(int i=d; i != arr.size(); ++i) &#123; int j = i; int tmp = arr[i]; for(; j &gt;= d &amp;&amp; tmp &lt; arr[j-d]; j -= d) arr[j] = arr[j-d]; arr[j] = tmp; &#125;&#125; 5.选择排序算法思想每趟排序在当前待排序序列中选取关键字最小或最大的记录,添加到有序序列中. 算法流程1、在待排序序列中找到最小（大）元素,存放到排序序列的末尾;2、重复步骤1,直到待排序序列只有一个元素,排序结束. 算法性能时间$O(n^2)$,空间$O(1)$ 排序不稳定 代码实现123456789101112131415161718192021222324void selectionSort(int left, int right, vector&lt;int&gt; &amp;arr)&#123; for(int i=left; i != right; ++i) &#123; int minPos=i; int minValue=arr[i]; for(int j = i + 1; j != right + 1; ++j) //找出当前序列中的最小元素; if(arr[j] &lt; minValue) &#123; minPos = j; minValue = arr[j]; &#125; if(minPos!=i) //将找到的最小元素放到已排序序列末端; &#123; arr[minPos] = arr[i]; arr[i] = minValue; &#125; &#125;&#125;void selectionSort(vector&lt;int&gt; &amp;arr)&#123; selectionSort(0, arr.size()-1, arr);&#125; 6.堆排序思考改进堆排序是对直接选择排序的改进,直接选择排序每一次排序是找出待排序序列中的关键字值最小的记录,如果在此同时也找出关键字值较小的记录,则可以减少后面选择中所用的比较次数,从而提高效率. 算法思想将待排序的记录序列用完全二叉树表示,然后构造最小堆或者最大堆,移除堆顶元素,再将剩余元素整理成堆,重复步骤直至堆中只有一个元素,则排序完成. 算法流程1、将待排序的记录序列用完全二叉树表示;2、初始化建堆,依次对n/2,…,2,1,0元素执行pushDown操作;3、交换堆顶元素与下标最大的叶结点,将剩余n-1个元素整理成堆;4、重复执行步骤3,直至堆中只有一个元素,则排序完成. 算法性能时间$O(nlogn)$,空间$O(1)$ 算法不稳定 代码实现1234567891011121314151617181920void heapSort(vector&lt;int&gt; &amp;arr)&#123; vector&lt;int&gt; dev(arr.size()+1); //个人习惯从下标为1开始建堆,这不是必须的操作,故不计此开销; for(int i=1; i!=arr.size()+1; ++i) //建立完全二叉树; dev[i]=arr[i-1]; for(int i=arr.size()/2; i&gt;=1; --i) //建立最小堆; pushDown(i,arr.size(),dev); for(int i=arr.size(); i&gt;=2; --i) //交换元素,将剩余元素整理成堆; &#123; int tmp = dev[1]; dev[1] = dev[i]; dev[i] = tmp; pushDown(1,i-1,dev); &#125; for(int i=1; i!=arr.size()+1; ++i) //复制回初始数组; arr[i-1]=dev[i];&#125; 7.归并排序算法思想将若干个有序序列逐步归并,最终得到一个有序序列. 算法流程1、自底向上 非递归算法 将一个具有n个记录的待排序序列看成n个长度为1的有序序列,进行两两归并,得到[n/2]个长度为2的有序序列,再进行两两归并,得到[n/4]个长度为4的有序序列,重复该步骤,直到得到一个长度为n的有序序列.2、自顶向下 分治算法(递归) 将待排序序列一分为二,分裂点mid=(low+high)/2,递归地对序列arr[low],…,arr[mid]和arr[mid+1],…,arr[high]进行归并排序,然后合并两个子序列.递归终止条件,子序列长度为1. 算法性能时间$O(nlogn)$,空间$O(n)$ 算法稳定 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//二路归并void merge(int start, int mid, int end, vector&lt;int&gt; &amp;arrA, vector&lt;int&gt; &amp;arrB)&#123; //注:此函数将合并段结果储存在arrB数组中; int i = start, j = mid + 1, k = start; while(i &lt;= mid &amp;&amp; j &lt;= end) arrB[k++] = arrA[i] &lt;= arrA[j] ? arrA[i++] : arrA[j++]; while(i &lt;= mid) arrB[k++] = arrA[i++]; while(j &lt;= end) arrB[k++] = arrA[j++];&#125;//自底向上的非递归算法void mergePass(int h, vector&lt;int&gt; &amp;arrA, vector&lt;int&gt; &amp;arrB)&#123; int i = 0; for(; i+2*h-1 &lt; arrA.size(); i += 2*h) merge(i, i+h-1, i+2*h-1, arrA, arrB); if(i+h-1 &lt; arrA.size()-1) //剩下元素个数大于一段,不足两段; merge(i, i+h-1, arrA.size()-1, arrA, arrB); else //剩下元素个数不足一段,直接拷贝; for(; i!=arrA.size(); ++i) arrB[i] = arrA[i];&#125;void mergeSort_1(vector&lt;int&gt; &amp;arr)&#123; int h = 1; vector&lt;int&gt; dev(arr.size()); while(h &lt; arr.size()) &#123; mergePass(h,arr,dev); //从数组arr归并到数组dev; h *= 2; mergePass(h,dev,arr); //从数组dev归并到数组arr; h *= 2; &#125;&#125;//自顶向下的分治算法void mergeSort_2(int low, int high, vector&lt;int&gt; &amp;arrA, vector&lt;int&gt; &amp;arrB)&#123; if(low &lt; high) &#123; int mid = low + (high - low) / 2; mergeSort_2(low, mid, arrA, arrB); mergeSort_2(mid+1, high, arrA, arrB); merge(low, mid, high, arrA, arrB); //二路归并; for(int i = low; i &lt;= high; ++i) //拷贝回原数组; arrA[i] = arrB[i]; &#125;&#125;void mergeSort_2(vector&lt;int&gt; &amp;arr)&#123; vector&lt;int&gt; dev(arr.size()); mergeSort_2(0, arr.size()-1, arr, dev);&#125; 8.计数排序算法思想假设n个输入元素都是在0到k区间内的一个整数,对每一个元素x,统计小于等于x的元素个数,包括其本身,利用这一信息可直接将元素输出到数组中的正确位置上. 算法流程1、初始化辅助数组count[k],统计元素出现次数,如元素i用count[i]表示;2、对于每一个元素,统计小于等于其本身的元素个数,即count[i]+=count[i-1];3、反向扫描待排序数组,每扫描一项,将其存入临时数组dev中的第count[i]项,count[i]减一;4、拷贝回原数组,排序完成. 算法性能时间$O(k+n)$,空间$O(k+n)$ 算法稳定 代码实现1234567891011121314151617181920212223void countingSort(vector&lt;int&gt; &amp;arr)&#123; //找出最大值; if(arr.empty())return; int max = arr[0]; for(int i=1; i != arr.size(); ++i) if(arr[i] &gt; max) max = arr[i]; //计数; vector&lt;int&gt; count(max+1,0); vector&lt;int&gt; dev(arr.size()+1); for(int i=0; i != arr.size(); ++i) count[arr[i]]++; for(int i=1; i &lt;= max; ++i) count[i] += count[i-1]; //将记录输出到正确位置,并复制回原数组; for(int i=arr.size()-1; i &gt;= 0; --i) dev[count[arr[i]]--] = arr[i]; for(int i=0; i != arr.size(); ++i) arr[i] = dev[i+1];&#125; 9.基数排序算法思想通过对关键字的每一个分量使用一种稳定的排序方法,从低位到高位对序列进行排序. 算法流程1、将待排序记录装入一个队列A,设置10个初始空队列vector]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>sort</tag>
      </tags>
  </entry>
</search>
